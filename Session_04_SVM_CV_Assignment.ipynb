{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pascal-maker/machinelearning/blob/main/Session_04_SVM_CV_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvFicl_vJQWI"
      },
      "source": [
        "# SVM & Cross-Validation\n",
        "\n",
        "In the realm of machine learning, Support Vector Machines (SVMs) stand out as a powerful tool for classification and regression tasks. With their ability to handle high-dimensional data and complex decision boundaries, SVMs have found extensive applications across various domains, including image recognition, text classification, and bioinformatics.\n",
        "\n",
        "Throughout this assignment, we aim to provide a comprehensive understanding of Support Vector Machines and their integration with cross-validation techniques, equipping you with the knowledge and skills necessary to apply these methods confidently in your own machine learning problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OuQj3Q7PJQWK"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "from scipy.stats import randint\n",
        "from scipy.stats import uniform\n",
        "from skimage.io import imread, imshow\n",
        "\n",
        "pd.set_option('display.max_rows',1000)\n",
        "pd.set_option('display.max_columns',1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "FZra3xFOU_Dk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgVppKsiJQWK"
      },
      "source": [
        "## 1. Cancer detection\n",
        "\n",
        "\n",
        "Train a model to predict whether a specific tumor is benign or malignant. Use the dataset 'cancer.csv' for this purpose.\n",
        "\n",
        "Base your approach on methodologies from previous assignments to achieve the best possible results. Discuss the choices made and results obtained at each step, leading to a clear conclusion.\n",
        "\n",
        "Tip: A classifier can only be trained with numerical values. Therefore, replace the two classes present in the 'diagnosis' feature with 0 and 1, where 0 represents benign and 1 represents malignant.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "076m2Ae_JQWK",
        "outputId": "7a01e324-6c31-4e8a-aa6c-ba0ab87123a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842517         M        20.57         17.77          132.90     1326.0   \n",
              "1  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "2  84348301         M        11.42         20.38           77.58      386.1   \n",
              "3  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "4    843786         M        12.45         15.70           82.57      477.1   \n",
              "5    844359         M        18.25         19.98          119.60     1040.0   \n",
              "6  84458202         M        13.71         20.83           90.20      577.9   \n",
              "7    844981         M        13.00         21.82           87.50      519.8   \n",
              "8  84501001         M        12.46         24.04           83.97      475.9   \n",
              "9    845636         M        16.02         23.24          102.70      797.8   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.08474           0.07864         0.08690              0.07017   \n",
              "1          0.10960           0.15990         0.19740              0.12790   \n",
              "2          0.14250           0.28390         0.24140              0.10520   \n",
              "3          0.10030           0.13280         0.19800              0.10430   \n",
              "4          0.12780           0.17000         0.15780              0.08089   \n",
              "5          0.09463           0.10900         0.11270              0.07400   \n",
              "6          0.11890           0.16450         0.09366              0.05985   \n",
              "7          0.12730           0.19320         0.18590              0.09353   \n",
              "8          0.11860           0.23960         0.22730              0.08543   \n",
              "9          0.08206           0.06669         0.03299              0.03323   \n",
              "\n",
              "   symmetry_mean  fractal_dimension_mean  radius_se  texture_se  perimeter_se  \\\n",
              "0         0.1812                 0.05667     0.5435      0.7339         3.398   \n",
              "1         0.2069                 0.05999     0.7456      0.7869         4.585   \n",
              "2         0.2597                 0.09744     0.4956      1.1560         3.445   \n",
              "3         0.1809                 0.05883     0.7572      0.7813         5.438   \n",
              "4         0.2087                 0.07613     0.3345      0.8902         2.217   \n",
              "5         0.1794                 0.05742     0.4467      0.7732         3.180   \n",
              "6         0.2196                 0.07451     0.5835      1.3770         3.856   \n",
              "7         0.2350                 0.07389     0.3063      1.0020         2.406   \n",
              "8         0.2030                 0.08243     0.2976      1.5990         2.039   \n",
              "9         0.1528                 0.05697     0.3795      1.1870         2.466   \n",
              "\n",
              "   area_se  smoothness_se  compactness_se  concavity_se  concave points_se  \\\n",
              "0    74.08       0.005225        0.013080       0.01860           0.013400   \n",
              "1    94.03       0.006150        0.040060       0.03832           0.020580   \n",
              "2    27.23       0.009110        0.074580       0.05661           0.018670   \n",
              "3    94.44       0.011490        0.024610       0.05688           0.018850   \n",
              "4    27.19       0.007510        0.033450       0.03672           0.011370   \n",
              "5    53.91       0.004314        0.013820       0.02254           0.010390   \n",
              "6    50.96       0.008805        0.030290       0.02488           0.014480   \n",
              "7    24.32       0.005731        0.035020       0.03553           0.012260   \n",
              "8    23.94       0.007149        0.072170       0.07743           0.014320   \n",
              "9    40.51       0.004029        0.009269       0.01101           0.007591   \n",
              "\n",
              "   symmetry_se  fractal_dimension_se  radius_worst  texture_worst  \\\n",
              "0      0.01389              0.003532         24.99          23.41   \n",
              "1      0.02250              0.004571         23.57          25.53   \n",
              "2      0.05963              0.009208         14.91          26.50   \n",
              "3      0.01756              0.005115         22.54          16.67   \n",
              "4      0.02165              0.005082         15.47          23.75   \n",
              "5      0.01369              0.002179         22.88          27.66   \n",
              "6      0.01486              0.005412         17.06          28.14   \n",
              "7      0.02143              0.003749         15.49          30.73   \n",
              "8      0.01789              0.010080         15.09          40.68   \n",
              "9      0.01460              0.003042         19.19          33.88   \n",
              "\n",
              "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
              "0           158.80      1956.0            0.1238             0.1866   \n",
              "1           152.50      1709.0            0.1444             0.4245   \n",
              "2            98.87       567.7            0.2098             0.8663   \n",
              "3           152.20      1575.0            0.1374             0.2050   \n",
              "4           103.40       741.6            0.1791             0.5249   \n",
              "5           153.20      1606.0            0.1442             0.2576   \n",
              "6           110.60       897.0            0.1654             0.3682   \n",
              "7           106.20       739.3            0.1703             0.5401   \n",
              "8            97.65       711.4            0.1853             1.0580   \n",
              "9           123.80      1150.0            0.1181             0.1551   \n",
              "\n",
              "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0           0.2416               0.18600          0.2750   \n",
              "1           0.4504               0.24300          0.3613   \n",
              "2           0.6869               0.25750          0.6638   \n",
              "3           0.4000               0.16250          0.2364   \n",
              "4           0.5355               0.17410          0.3985   \n",
              "5           0.3784               0.19320          0.3063   \n",
              "6           0.2678               0.15560          0.3196   \n",
              "7           0.5390               0.20600          0.4378   \n",
              "8           1.1050               0.22100          0.4366   \n",
              "9           0.1459               0.09975          0.2948   \n",
              "\n",
              "   fractal_dimension_worst  \n",
              "0                  0.08902  \n",
              "1                  0.08758  \n",
              "2                  0.17300  \n",
              "3                  0.07678  \n",
              "4                  0.12440  \n",
              "5                  0.08368  \n",
              "6                  0.11510  \n",
              "7                  0.10720  \n",
              "8                  0.20750  \n",
              "9                  0.08452  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99a5d879-a38d-4c20-bd99-e4bc1c23dbc9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.18600</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.040060</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.020580</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.24300</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.074580</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.018670</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.25750</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.018850</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.16250</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.15780</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>0.2087</td>\n",
              "      <td>0.07613</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.8902</td>\n",
              "      <td>2.217</td>\n",
              "      <td>27.19</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.033450</td>\n",
              "      <td>0.03672</td>\n",
              "      <td>0.011370</td>\n",
              "      <td>0.02165</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.17410</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>844359</td>\n",
              "      <td>M</td>\n",
              "      <td>18.25</td>\n",
              "      <td>19.98</td>\n",
              "      <td>119.60</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>0.09463</td>\n",
              "      <td>0.10900</td>\n",
              "      <td>0.11270</td>\n",
              "      <td>0.07400</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.05742</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>3.180</td>\n",
              "      <td>53.91</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.013820</td>\n",
              "      <td>0.02254</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.01369</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>22.88</td>\n",
              "      <td>27.66</td>\n",
              "      <td>153.20</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.3784</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.08368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>84458202</td>\n",
              "      <td>M</td>\n",
              "      <td>13.71</td>\n",
              "      <td>20.83</td>\n",
              "      <td>90.20</td>\n",
              "      <td>577.9</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.16450</td>\n",
              "      <td>0.09366</td>\n",
              "      <td>0.05985</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07451</td>\n",
              "      <td>0.5835</td>\n",
              "      <td>1.3770</td>\n",
              "      <td>3.856</td>\n",
              "      <td>50.96</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.030290</td>\n",
              "      <td>0.02488</td>\n",
              "      <td>0.014480</td>\n",
              "      <td>0.01486</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>17.06</td>\n",
              "      <td>28.14</td>\n",
              "      <td>110.60</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.1654</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.2678</td>\n",
              "      <td>0.15560</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.11510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>844981</td>\n",
              "      <td>M</td>\n",
              "      <td>13.00</td>\n",
              "      <td>21.82</td>\n",
              "      <td>87.50</td>\n",
              "      <td>519.8</td>\n",
              "      <td>0.12730</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.18590</td>\n",
              "      <td>0.09353</td>\n",
              "      <td>0.2350</td>\n",
              "      <td>0.07389</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>1.0020</td>\n",
              "      <td>2.406</td>\n",
              "      <td>24.32</td>\n",
              "      <td>0.005731</td>\n",
              "      <td>0.035020</td>\n",
              "      <td>0.03553</td>\n",
              "      <td>0.012260</td>\n",
              "      <td>0.02143</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>15.49</td>\n",
              "      <td>30.73</td>\n",
              "      <td>106.20</td>\n",
              "      <td>739.3</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.5401</td>\n",
              "      <td>0.5390</td>\n",
              "      <td>0.20600</td>\n",
              "      <td>0.4378</td>\n",
              "      <td>0.10720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>84501001</td>\n",
              "      <td>M</td>\n",
              "      <td>12.46</td>\n",
              "      <td>24.04</td>\n",
              "      <td>83.97</td>\n",
              "      <td>475.9</td>\n",
              "      <td>0.11860</td>\n",
              "      <td>0.23960</td>\n",
              "      <td>0.22730</td>\n",
              "      <td>0.08543</td>\n",
              "      <td>0.2030</td>\n",
              "      <td>0.08243</td>\n",
              "      <td>0.2976</td>\n",
              "      <td>1.5990</td>\n",
              "      <td>2.039</td>\n",
              "      <td>23.94</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.072170</td>\n",
              "      <td>0.07743</td>\n",
              "      <td>0.014320</td>\n",
              "      <td>0.01789</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>15.09</td>\n",
              "      <td>40.68</td>\n",
              "      <td>97.65</td>\n",
              "      <td>711.4</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>1.0580</td>\n",
              "      <td>1.1050</td>\n",
              "      <td>0.22100</td>\n",
              "      <td>0.4366</td>\n",
              "      <td>0.20750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>845636</td>\n",
              "      <td>M</td>\n",
              "      <td>16.02</td>\n",
              "      <td>23.24</td>\n",
              "      <td>102.70</td>\n",
              "      <td>797.8</td>\n",
              "      <td>0.08206</td>\n",
              "      <td>0.06669</td>\n",
              "      <td>0.03299</td>\n",
              "      <td>0.03323</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.05697</td>\n",
              "      <td>0.3795</td>\n",
              "      <td>1.1870</td>\n",
              "      <td>2.466</td>\n",
              "      <td>40.51</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>0.009269</td>\n",
              "      <td>0.01101</td>\n",
              "      <td>0.007591</td>\n",
              "      <td>0.01460</td>\n",
              "      <td>0.003042</td>\n",
              "      <td>19.19</td>\n",
              "      <td>33.88</td>\n",
              "      <td>123.80</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>0.1181</td>\n",
              "      <td>0.1551</td>\n",
              "      <td>0.1459</td>\n",
              "      <td>0.09975</td>\n",
              "      <td>0.2948</td>\n",
              "      <td>0.08452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99a5d879-a38d-4c20-bd99-e4bc1c23dbc9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99a5d879-a38d-4c20-bd99-e4bc1c23dbc9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99a5d879-a38d-4c20-bd99-e4bc1c23dbc9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6aba94bf-fb5a-4c3b-a51b-a815609b4205\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6aba94bf-fb5a-4c3b-a51b-a815609b4205')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6aba94bf-fb5a-4c3b-a51b-a815609b4205 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Reading the dataset\n",
        "\n",
        "dataset = pd.read_csv('cancer.csv')\n",
        "dataset.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "h8qVkoZ1JQWL",
        "outputId": "74007f56-369c-4cb3-ece1-551bccb36d02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='diagnosis', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Check whether or not the dataset is imbalanced\n",
        "sns.countplot(x='diagnosis',data=dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Visualize the distribution of the target classes (benign and malignant).\n",
        "\n",
        "Important because class imbalance can significantly impact model performance."
      ],
      "metadata": {
        "id": "JXRpfMqk-PyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.drop('id',axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "N50OtdhWMUut"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yes -> 1 and no -> 0\n",
        "\n",
        "dataset.diagnosis.replace({'B':0,'M':1},inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRGCV5OOMnNi",
        "outputId": "e343ace0-eadd-4658-ea22-fd8fef45dda8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b39a50d46acb>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  dataset.diagnosis.replace({'B':0,'M':1},inplace=True)\n",
            "<ipython-input-5-b39a50d46acb>:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  dataset.diagnosis.replace({'B':0,'M':1},inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iWDsKEI1JQWL"
      },
      "outputs": [],
      "source": [
        "# Drop the ID column\n",
        "\n",
        "\n",
        "# Replace B by 0 and M by 1 in the diagnosis column\n",
        "\n",
        "\n",
        "\n",
        "# Split into features and targets\n",
        "\n",
        "X = dataset.drop('diagnosis',axis=1)\n",
        "y = dataset['diagnosis']\n",
        "\n",
        "\n",
        "\n",
        "# Split into training set and test set. Make sure that 150 samples end up in the test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=150, random_state=0)\n",
        "\n",
        "\n",
        "# To avoid a conversion warning during scaling:\n",
        "\n",
        "X_train = X_train.astype('float64')\n",
        "X_test = X_test.astype('float64')\n",
        "\n",
        "# Scaling\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Standardize the feature values to have zero mean and unit variance.\n",
        "\n",
        "This helps many machine learning algorithms (like SVM and Logistic Regression) perform better.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "Create training and test sets.\n",
        "\n",
        "Test set size is fixed at 150 to ensure consistent evaluation."
      ],
      "metadata": {
        "id": "hCesC7Xv--Cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Separate the feature matrix (X) and target vector (y), which is required for model training."
      ],
      "metadata": {
        "id": "o2scuvra-t30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Remove the ID column, which is not relevant for prediction.\n",
        "\n",
        "Convert the diagnosis column into numerical labels for binary classification"
      ],
      "metadata": {
        "id": "FX6FSeIV-mVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "model = LogisticRegression(max_iter=1)\n",
        "\n",
        "paramaters = [\n",
        "    {\n",
        "        'C': np.linspace(0.001, 100, 7),\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "]\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=paramaters,\n",
        "    scoring='accuracy',\n",
        "    cv=20,\n",
        "    n_jobs=-1,\n",
        "    verbose=5\n",
        ")\n",
        "\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best accuracy:', best_accuracy)\n",
        "print('Best parameters:', best_parameters)\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7YyrvFQqzD1",
        "outputId": "ca816fba-c057-46a0-df12-365c2b3401a7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 20 folds for each of 28 candidates, totalling 560 fits\n",
            "Best accuracy: 0.834\n",
            "Best parameters: {'C': np.float64(0.001), 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      1195\n",
            "           1       0.79      0.98      0.87      1352\n",
            "           2       0.87      0.81      0.84      1157\n",
            "           3       0.83      0.83      0.83      1258\n",
            "           4       0.84      0.89      0.86      1140\n",
            "           5       0.84      0.69      0.76      1076\n",
            "           6       0.88      0.92      0.90      1167\n",
            "           7       0.85      0.86      0.85      1268\n",
            "           8       0.86      0.70      0.77      1174\n",
            "           9       0.81      0.76      0.79      1213\n",
            "\n",
            "    accuracy                           0.84     12000\n",
            "   macro avg       0.85      0.84      0.84     12000\n",
            "weighted avg       0.84      0.84      0.84     12000\n",
            "\n",
            "[[1147    0    9    2    5    6   19    0    5    2]\n",
            " [   0 1327    3    2    0    9    2    1    8    0]\n",
            " [  23   43  941   33   20    5   45   19   24    4]\n",
            " [   5   33   56 1042    1   24   16   40   19   22]\n",
            " [   1   28    9    1 1013   10   10    3    9   56]\n",
            " [  29   36    6   97   34  743   42    9   52   28]\n",
            " [  27   17   15    0   15   14 1070    0    9    0]\n",
            " [   8   43   13    8   27    3    0 1089    1   76]\n",
            " [  18  140   14   46   22   66    9    4  826   29]\n",
            " [  22   19   11   28   70    8    0  122    7  926]]\n",
            "84.36666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cell Breakdown: Logistic Regression with Grid Search**\n",
        "\n",
        "This code cell is performing hyperparameter tuning for a **Logistic Regression** model using **GridSearchCV**. It systematically searches for the best combination of hyperparameters to maximize the model's performance. Let's break it down step by step.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "```\n",
        "\n",
        "* **GridSearchCV:** Performs exhaustive search over specified hyperparameter values for an estimator.\n",
        "* **LogisticRegression:** The machine learning model being tuned.\n",
        "* **numpy:** Used for numerical operations, specifically generating a range of **C** values.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Model Initialization**\n",
        "\n",
        "```python\n",
        "model = LogisticRegression(max_iter=1)\n",
        "```\n",
        "\n",
        "* **LogisticRegression(max\\_iter=1):** Initializes the logistic regression model.\n",
        "* **max\\_iter=1:** This is likely a mistake, as it limits the optimization to just one iteration, potentially preventing convergence. Normally, a much higher value (e.g., **100** or **1000**) is used.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Defining the Hyperparameter Grid**\n",
        "\n",
        "```python\n",
        "paramaters = [\n",
        "    {\n",
        "        'C': np.linspace(0.001, 100, 7),\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "* **C:** Regularization strength (inverse of regularization). Smaller values mean stronger regularization.\n",
        "* **np.linspace(0.001, 100, 7):** Generates 7 evenly spaced values from **0.001** to **100**.\n",
        "* **penalty:** Specifies the type of regularization to use.\n",
        "\n",
        "  * **l1:** Lasso (L1) regularization, promotes sparsity by driving some coefficients to zero.\n",
        "  * **l2:** Ridge (L2) regularization, prevents overfitting by penalizing large coefficients.\n",
        "* **solver:** Optimization algorithm used to find the best weights.\n",
        "\n",
        "  * **liblinear:** Suitable for smaller datasets, supports both **l1** and **l2**.\n",
        "  * **saga:** More suitable for larger datasets, supports **l1**, **l2**, and **elastic-net**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Setting Up Grid Search**\n",
        "\n",
        "```python\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=paramaters,\n",
        "    scoring='accuracy',\n",
        "    cv=20,\n",
        "    n_jobs=-1,\n",
        "    verbose=5\n",
        ")\n",
        "```\n",
        "\n",
        "* **estimator=model:** The logistic regression model to tune.\n",
        "* **param\\_grid=paramaters:** The hyperparameter grid defined earlier.\n",
        "* **scoring='accuracy':** Evaluates the model based on accuracy during cross-validation.\n",
        "* **cv=20:** 20-fold cross-validation, meaning the data is split into 20 parts, with each part being used once as a test set.\n",
        "* **n\\_jobs=-1:** Uses all available CPU cores for parallel processing, speeding up the grid search.\n",
        "* **verbose=5:** Prints detailed messages about the progress of the search.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Fitting the Grid Search Model**\n",
        "\n",
        "```python\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Trains the **LogisticRegression** model on the training set using each combination of hyperparameters from the grid.\n",
        "* Evaluates each combination using 20-fold cross-validation to identify the best one.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Extracting the Best Model and Parameters**\n",
        "\n",
        "```python\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best accuracy:', best_accuracy)\n",
        "print('Best parameters:', best_parameters)\n",
        "```\n",
        "\n",
        "* **best\\_score\\_:** Returns the highest cross-validated accuracy achieved during the grid search.\n",
        "* **best\\_params\\_:** Returns the set of hyperparameters that resulted in the highest accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Making Predictions on the Test Set**\n",
        "\n",
        "```python\n",
        "y_pred = grid_search.predict(X_test)\n",
        "```\n",
        "\n",
        "* Uses the best model identified during grid search to make predictions on the unseen test data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Evaluating the Model's Test Performance**\n",
        "\n",
        "```python\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* **classification\\_report:** Provides precision, recall, F1-score, and support for each class.\n",
        "* This is crucial for understanding how the model performs not just overall, but specifically for each class (e.g., benign vs malignant).\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. Confusion Matrix and Final Accuracy Check**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:** Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
        "* **accuracy\\_score:** Computes the overall accuracy as a percentage.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations:**\n",
        "\n",
        "1. **Regularization Impact:** The choice of **C** impacts the regularization strength, with smaller values leading to more regularization (simpler model) and larger values allowing more complex decision boundaries.\n",
        "\n",
        "2. **Solver Selection:** **liblinear** is generally faster for smaller datasets, while **saga** scales better to larger ones and supports **l1** regularization.\n",
        "\n",
        "3. **High Cross-Validation (CV) Setting:** Using **cv=20** increases the reliability of the estimated accuracy but also significantly increases computation time.\n",
        "\n",
        "4. **Potential Issue:** **max\\_iter=1** is too low and may prevent the model from converging, potentially leading to suboptimal results. It should be increased.\n",
        "\n",
        "---\n",
        "\n",
        "### **Suggested Improvements:**\n",
        "\n",
        "* **Increase `max_iter`:** Use a larger value, like **1000**, to ensure the model can fully converge.\n",
        "* **Use Class Weight Balancing:** Consider adding **class\\_weight='balanced'** to handle imbalanced classes more effectively.\n",
        "* **Optimize the Search Space:** Use a more refined grid for **C** or consider using **RandomizedSearchCV** if computation time is a concern.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cGIY-qWL_pnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Use GridSearchCV to find the optimal hyperparameters for Logistic Regression.\n",
        "\n",
        "Evaluates multiple combinations of C, penalty, and solver.\n",
        "\n",
        "Uses 20-fold cross-validation for a robust estimate of model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "L3_ww9S-_Lkk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewYUP6NJQWL"
      },
      "source": [
        "Train a logistic regression model via cross-validation.\n",
        "\n",
        "Utilize grid search and random search to find the best hyperparameters: C value, class_weight, penalty (l1 or l2). More information: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html.\n",
        "\n",
        "Vary the value of K in K-fold cross-validation. Discuss the results.\n",
        "Test the obtained models on the test set. Which search technique do you prefer and why?\n",
        "\n",
        "Does it make sense to expand the feature set with polynomial features? Test this. Polynomial Features: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSlmQRrQJQWL",
        "outputId": "51cdf015-62df-4545-827a-ddc3189219c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best SVM Parameters: {'C': np.float64(9591.748187665113), 'gamma': np.float64(0.03652560377611924), 'kernel': 'poly'}\n",
            "Best SVM Accuracy: 0.8985000000000001\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression Parameters: {'C': np.float64(531.606243116617), 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Logistic Regression Accuracy: 0.817\n",
            "\n",
            "Best Model Test Set Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      1195\n",
            "           1       0.98      0.99      0.98      1352\n",
            "           2       0.94      0.87      0.90      1157\n",
            "           3       0.95      0.86      0.91      1258\n",
            "           4       0.87      0.96      0.91      1140\n",
            "           5       0.90      0.91      0.90      1076\n",
            "           6       0.97      0.94      0.95      1167\n",
            "           7       0.95      0.87      0.91      1268\n",
            "           8       0.79      0.94      0.86      1174\n",
            "           9       0.87      0.89      0.88      1213\n",
            "\n",
            "    accuracy                           0.92     12000\n",
            "   macro avg       0.92      0.92      0.92     12000\n",
            "weighted avg       0.92      0.92      0.92     12000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1140    0    4    1    4   10   11    0   25    0]\n",
            " [   0 1334    3    1    1    4    0    0    7    2]\n",
            " [   9    3 1008   14   30    1    5    5   77    5]\n",
            " [   4    2   24 1087    3   37    3   11   73   14]\n",
            " [   3    4    4    0 1089    3    1    1    5   30]\n",
            " [   3    0    3   25    6  974   10    1   42   12]\n",
            " [   3    2    9    0   22   14 1092    1   24    0]\n",
            " [   2    6    5    1   33    2    0 1109   15   95]\n",
            " [   2    8    6    6    9   33    3    0 1100    7]\n",
            " [   4    5    8    7   58    5    0   35   17 1074]]\n",
            "Test Accuracy: 91.725\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# SVM Random Search\n",
        "svm_model = SVC()\n",
        "svm_params = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': uniform(0.001, 10000),\n",
        "    'gamma': uniform(0.001, 0.2)\n",
        "}\n",
        "\n",
        "svm_search = RandomizedSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_distributions=svm_params,\n",
        "    cv=5,\n",
        "    n_iter=50,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "svm_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best SVM Parameters:\", svm_search.best_params_)\n",
        "print(\"Best SVM Accuracy:\", svm_search.best_score_)\n",
        "\n",
        "# Logistic Regression Random Search\n",
        "logreg_model = LogisticRegression(max_iter=1, tol=1e-2)\n",
        "logreg_params = {\n",
        "    'C': uniform(0.01, 1000),\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "logreg_search = RandomizedSearchCV(\n",
        "    estimator=logreg_model,\n",
        "    param_distributions=logreg_params,\n",
        "    cv=5,\n",
        "    n_iter=50,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "logreg_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Logistic Regression Parameters:\", logreg_search.best_params_)\n",
        "print(\"Best Logistic Regression Accuracy:\", logreg_search.best_score_)\n",
        "\n",
        "# Test set performance for the best model\n",
        "best_model = svm_search if svm_search.best_score_ > logreg_search.best_score_ else logreg_search\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nBest Model Test Set Performance:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cell Breakdown: Randomized Hyperparameter Search for SVM and Logistic Regression**\n",
        "\n",
        "This cell performs **RandomizedSearchCV** for both **Support Vector Machines (SVM)** and **Logistic Regression** to find the best hyperparameter combinations, then evaluates the best model on a test set. It is an efficient approach to hyperparameter tuning, as it samples a specified number of random parameter combinations instead of testing all possible ones (as with **GridSearchCV**).\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import uniform\n",
        "```\n",
        "\n",
        "* **RandomizedSearchCV:** Efficient hyperparameter search over random combinations.\n",
        "* **SVC:** Support Vector Machine model for classification.\n",
        "* **LogisticRegression:** Logistic Regression model for classification.\n",
        "* **uniform:** Generates continuous random numbers over a specified range, used for hyperparameter tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. SVM Hyperparameter Search Setup**\n",
        "\n",
        "```python\n",
        "# SVM Random Search\n",
        "svm_model = SVC()\n",
        "svm_params = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': uniform(0.001, 10000),\n",
        "    'gamma': uniform(0.001, 0.2)\n",
        "}\n",
        "```\n",
        "\n",
        "* **SVM Model Initialization:**\n",
        "\n",
        "  * Creates an **SVC** object for the SVM model.\n",
        "* **Parameter Space (`svm_params`):**\n",
        "\n",
        "  * **kernel:** Chooses between different kernel functions.\n",
        "\n",
        "    * **linear:** Simple linear hyperplane.\n",
        "    * **rbf:** Radial basis function, good for non-linear decision boundaries.\n",
        "    * **poly:** Polynomial kernel, introduces more complex decision boundaries.\n",
        "  * **C:** Regularization parameter. Controls the trade-off between a smooth decision boundary and classifying training points correctly.\n",
        "  * **gamma:** Defines how far the influence of a single training example reaches. Low values mean broader influence, while high values make the influence more localized.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Initializing the SVM Randomized Search**\n",
        "\n",
        "```python\n",
        "svm_search = RandomizedSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_distributions=svm_params,\n",
        "    cv=5,\n",
        "    n_iter=50,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* **estimator:** The SVM model to optimize.\n",
        "* **param\\_distributions:** The parameter space defined above.\n",
        "* **cv=5:** 5-fold cross-validation to evaluate each parameter combination.\n",
        "* **n\\_iter=50:** Tests 50 random combinations of the hyperparameters.\n",
        "* **n\\_jobs=-1:** Uses all available CPU cores for faster training.\n",
        "* **verbose=1:** Provides detailed output during training.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Fitting the SVM Randomized Search**\n",
        "\n",
        "```python\n",
        "svm_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Fits the SVM model to the training data with 5-fold cross-validation for each random combination of parameters.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Printing the Best SVM Model Parameters**\n",
        "\n",
        "```python\n",
        "print(\"Best SVM Parameters:\", svm_search.best_params_)\n",
        "print(\"Best SVM Accuracy:\", svm_search.best_score_)\n",
        "```\n",
        "\n",
        "* **best\\_params\\_:** The set of parameters that gave the highest cross-validated accuracy.\n",
        "* **best\\_score\\_:** The highest average cross-validated accuracy achieved.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Logistic Regression Hyperparameter Search Setup**\n",
        "\n",
        "```python\n",
        "# Logistic Regression Random Search\n",
        "logreg_model = LogisticRegression(max_iter=1, tol=1e-2)\n",
        "logreg_params = {\n",
        "    'C': uniform(0.01, 1000),\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "```\n",
        "\n",
        "* **Model Initialization:**\n",
        "\n",
        "  * **max\\_iter=1:** Limits the number of training iterations, which is too low and likely a mistake (should be higher, like 1000).\n",
        "  * **tol=1e-2:** Sets the tolerance for stopping criteria (higher tolerance means faster but potentially less accurate convergence).\n",
        "* **Parameter Space (`logreg_params`):**\n",
        "\n",
        "  * **C:** Regularization parameter. Smaller values imply stronger regularization.\n",
        "  * **solver:** Optimization algorithm.\n",
        "\n",
        "    * **liblinear:** Suitable for small datasets, supports **l1** and **l2** penalties.\n",
        "    * **saga:** Efficient for larger datasets, supports **l1**, **l2**, and **elastic-net**.\n",
        "  * **penalty:** Regularization type.\n",
        "\n",
        "    * **l1:** Lasso, drives some coefficients to zero for feature selection.\n",
        "    * **l2:** Ridge, penalizes large coefficients to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Initializing the Logistic Regression Randomized Search**\n",
        "\n",
        "```python\n",
        "logreg_search = RandomizedSearchCV(\n",
        "    estimator=logreg_model,\n",
        "    param_distributions=logreg_params,\n",
        "    cv=5,\n",
        "    n_iter=50,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* **estimator:** The logistic regression model to optimize.\n",
        "* **param\\_distributions:** The parameter space defined above.\n",
        "* **cv=5:** 5-fold cross-validation.\n",
        "* **n\\_iter=50:** Tests 50 random parameter combinations.\n",
        "* **n\\_jobs=-1:** Uses all available CPU cores for faster training.\n",
        "* **verbose=1:** Detailed training logs.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Fitting the Logistic Regression Randomized Search**\n",
        "\n",
        "```python\n",
        "logreg_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Fits the logistic regression model to the training data with 5-fold cross-validation for each random combination of parameters.\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. Printing the Best Logistic Regression Model Parameters**\n",
        "\n",
        "```python\n",
        "print(\"Best Logistic Regression Parameters:\", logreg_search.best_params_)\n",
        "print(\"Best Logistic Regression Accuracy:\", logreg_search.best_score_)\n",
        "```\n",
        "\n",
        "* **best\\_params\\_:** Returns the set of parameters that yielded the highest cross-validated accuracy.\n",
        "* **best\\_score\\_:** Returns the highest average cross-validated accuracy achieved.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. Selecting the Best Model Based on Validation Accuracy**\n",
        "\n",
        "```python\n",
        "# Test set performance for the best model\n",
        "best_model = svm_search if svm_search.best_score_ > logreg_search.best_score_ else logreg_search\n",
        "```\n",
        "\n",
        "* Chooses the model (either **SVM** or **Logistic Regression**) that achieved the highest cross-validated accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "#### **11. Testing the Best Model on the Unseen Test Data**\n",
        "\n",
        "```python\n",
        "y_pred = best_model.predict(X_test)\n",
        "```\n",
        "\n",
        "* Uses the best model to make predictions on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### **12. Evaluating Test Set Performance**\n",
        "\n",
        "```python\n",
        "print(\"\\nBest Model Test Set Performance:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* **classification\\_report:** Shows precision, recall, F1-score, and support for each class.\n",
        "* **confusion\\_matrix:** Provides the count of true positives, true negatives, false positives, and false negatives.\n",
        "* **accuracy\\_score:** Calculates the overall accuracy as a percentage.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations:**\n",
        "\n",
        "1. **Flexibility:** This approach can find the best model between SVM and Logistic Regression in a single run.\n",
        "2. **Potential Issue:** The **max\\_iter=1** for Logistic Regression is too low and might prevent convergence.\n",
        "3. **Scalability:** Both searches use **n\\_jobs=-1**, which significantly speeds up training.\n",
        "4. **Balanced Performance:** Both models have the ability to handle regularization, which helps reduce overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Suggested Improvements:**\n",
        "\n",
        "* **Increase `max_iter`:** Use a higher value (e.g., **1000**) for Logistic Regression.\n",
        "* **Class Balancing:** Consider adding **class\\_weight='balanced'** for better handling of imbalanced datasets.\n",
        "* **Scoring Metrics:** Use balanced accuracy or F1-micro if class imbalance is significant.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jMdys7Ab_7rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Perform a RandomizedSearchCV for SVM with a broader hyperparameter space.\n",
        "\n",
        "Faster than GridSearchCV for high-dimensional hyperparameter spaces.\n",
        "\n"
      ],
      "metadata": {
        "id": "TgzUW_Xb_Rip"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPqy9kOyJQWL"
      },
      "source": [
        "Re-train the grid search again but select the model that yields the highest balanced accuracy.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\n",
        "Also check http://mvpa.blogspot.com/2015/12/balanced-accuracy-what-and-why.html\n",
        "\n",
        "Discuss the result and compare it with the best accuracy score achieved earlier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# SVM Model\n",
        "model = SVC()\n",
        "\n",
        "# Hyperparameter Space\n",
        "parameters = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': uniform(0.001, 10000),\n",
        "    'gamma': uniform(0.001, 0.2)\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV with Balanced Accuracy\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=parameters,\n",
        "    cv=5,\n",
        "    n_iter=n_iter_search,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    scoring='balanced_accuracy'\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_balanced_accuracy = random_search.best_score_\n",
        "best_parameters = random_search.best_params_\n",
        "\n",
        "print('Best balanced accuracy:', best_balanced_accuracy)\n",
        "print('Best parameters:', best_parameters)\n",
        "\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred) * 100)\n",
        "print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOJzDlTJuz42",
        "outputId": "cc63b15d-5425-42b2-ffec-4ed3fb780a03"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best balanced accuracy: 0.8977766075010887\n",
            "Best parameters: {'C': np.float64(4059.989820260259), 'gamma': np.float64(0.11738560322024161), 'kernel': 'poly'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      1195\n",
            "           1       0.98      0.99      0.98      1352\n",
            "           2       0.94      0.87      0.90      1157\n",
            "           3       0.95      0.86      0.91      1258\n",
            "           4       0.87      0.96      0.91      1140\n",
            "           5       0.90      0.91      0.90      1076\n",
            "           6       0.97      0.94      0.95      1167\n",
            "           7       0.95      0.87      0.91      1268\n",
            "           8       0.79      0.94      0.86      1174\n",
            "           9       0.87      0.89      0.88      1213\n",
            "\n",
            "    accuracy                           0.92     12000\n",
            "   macro avg       0.92      0.92      0.92     12000\n",
            "weighted avg       0.92      0.92      0.92     12000\n",
            "\n",
            "[[1140    0    4    1    4   10   11    0   25    0]\n",
            " [   0 1334    3    1    1    4    0    0    7    2]\n",
            " [   9    3 1008   14   30    1    5    5   77    5]\n",
            " [   4    2   24 1087    3   37    3   11   73   14]\n",
            " [   3    4    4    0 1089    3    1    1    5   30]\n",
            " [   3    0    3   25    6  974   10    1   42   12]\n",
            " [   3    2    9    0   22   14 1092    1   24    0]\n",
            " [   2    6    5    1   33    2    0 1109   15   95]\n",
            " [   2    8    6    6    9   33    3    0 1100    7]\n",
            " [   4    5    8    7   58    5    0   35   17 1074]]\n",
            "Test Accuracy: 91.725\n",
            "Balanced Accuracy Score: 0.9169131563286264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Breakdown: RandomizedSearchCV with Balanced Accuracy for SVM**\n",
        "\n",
        "This code uses **RandomizedSearchCV** to optimize a **Support Vector Machine (SVM)** model using **balanced accuracy** as the scoring metric. This is particularly important when dealing with imbalanced datasets, where standard accuracy can be misleading.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "```\n",
        "\n",
        "* **RandomizedSearchCV:** Efficient hyperparameter search over random combinations.\n",
        "* **SVC:** Support Vector Machine model for classification.\n",
        "* **uniform:** Generates continuous random numbers over a specified range, used for hyperparameter tuning.\n",
        "* **balanced\\_accuracy\\_score:** Evaluates the balanced accuracy, which accounts for class imbalance by averaging the recall for each class.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Initializing the SVM Model**\n",
        "\n",
        "```python\n",
        "# SVM Model\n",
        "model = SVC()\n",
        "```\n",
        "\n",
        "* Initializes a **Support Vector Classifier (SVC)** with default parameters.\n",
        "* **SVC** is suitable for both binary and multiclass classification and supports various kernel functions for nonlinear decision boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Defining the Hyperparameter Search Space**\n",
        "\n",
        "```python\n",
        "# Hyperparameter Space\n",
        "parameters = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': uniform(0.001, 10000),\n",
        "    'gamma': uniform(0.001, 0.2)\n",
        "}\n",
        "```\n",
        "\n",
        "* **kernel:** Type of decision boundary.\n",
        "\n",
        "  * **linear:** Creates a straight line or hyperplane.\n",
        "  * **rbf:** Radial basis function, suitable for complex, non-linear boundaries.\n",
        "  * **poly:** Polynomial kernel, allows for curved decision boundaries.\n",
        "* **C (Regularization Parameter):**\n",
        "\n",
        "  * Controls the trade-off between achieving a low training error and a low testing error.\n",
        "  * A high **C** means the model will try to fit the training data as closely as possible, risking overfitting.\n",
        "* **gamma (Kernel Coefficient):**\n",
        "\n",
        "  * Controls the influence of each training example.\n",
        "  * Low values make the decision boundary smoother (more generalized), while high values lead to tighter, more complex boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Setting Up the RandomizedSearchCV**\n",
        "\n",
        "```python\n",
        "# RandomizedSearchCV with Balanced Accuracy\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=parameters,\n",
        "    cv=5,\n",
        "    n_iter=n_iter_search,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    scoring='balanced_accuracy'\n",
        ")\n",
        "```\n",
        "\n",
        "* **model:** The **SVC** model to be optimized.\n",
        "* **param\\_distributions:** The hyperparameter space defined earlier.\n",
        "* **cv=5:** 5-fold cross-validation.\n",
        "* **n\\_iter=10:** Tests 10 random combinations of the hyperparameters.\n",
        "* **n\\_jobs=-1:** Utilizes all available CPU cores for faster training.\n",
        "* **verbose=1:** Provides detailed output during training.\n",
        "* **scoring='balanced\\_accuracy':** Uses balanced accuracy, which is particularly important for imbalanced datasets because it gives equal weight to each class.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Fitting the RandomizedSearchCV**\n",
        "\n",
        "```python\n",
        "random_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Trains the SVM model on the training data.\n",
        "* Evaluates each random combination of hyperparameters using 5-fold cross-validation, optimizing for **balanced accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Extracting the Best Parameters and Score**\n",
        "\n",
        "```python\n",
        "best_balanced_accuracy = random_search.best_score_\n",
        "best_parameters = random_search.best_params_\n",
        "\n",
        "print('Best balanced accuracy:', best_balanced_accuracy)\n",
        "print('Best parameters:', best_parameters)\n",
        "```\n",
        "\n",
        "* **best\\_score\\_:** The highest cross-validated balanced accuracy achieved.\n",
        "* **best\\_params\\_:** The set of hyperparameters that resulted in the best balanced accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Testing the Best Model on Unseen Data**\n",
        "\n",
        "```python\n",
        "y_pred = random_search.predict(X_test)\n",
        "```\n",
        "\n",
        "* Uses the best model found during the randomized search to make predictions on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Evaluating the Test Set Performance**\n",
        "\n",
        "```python\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* **classification\\_report:** Provides precision, recall, F1-score, and support for each class, offering a comprehensive view of the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. Generating the Confusion Matrix**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:** Outputs a matrix showing the counts of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. Printing the Overall Test Accuracy and Balanced Accuracy**\n",
        "\n",
        "```python\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred) * 100)\n",
        "print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* **accuracy\\_score:** Provides the overall accuracy as a percentage, without accounting for class imbalance.\n",
        "* **balanced\\_accuracy\\_score:** The more appropriate metric for imbalanced datasets, calculated as the average of recall for each class.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations:**\n",
        "\n",
        "1. **Balanced Accuracy Advantage:**\n",
        "\n",
        "   * This approach specifically optimizes for **balanced accuracy**, which is crucial when the classes are imbalanced (e.g., rare disease detection).\n",
        "2. **Random Search Efficiency:**\n",
        "\n",
        "   * RandomizedSearchCV is much faster than GridSearchCV for large hyperparameter spaces, as it samples a fixed number of combinations.\n",
        "3. **Risk of Overfitting:**\n",
        "\n",
        "   * High values of **C** can lead to overfitting, especially if the dataset has noise or outliers.\n",
        "4. **Scalability:**\n",
        "\n",
        "   * Using **n\\_jobs=-1** ensures faster model fitting by parallelizing the training process.\n",
        "\n",
        "---\n",
        "\n",
        "### **Potential Improvements:**\n",
        "\n",
        "* **Refining Hyperparameter Space:** Use a more precise distribution for **C** and **gamma** to avoid extreme values.\n",
        "* **Adding Class Weights:** Consider adding **class\\_weight='balanced'** to further improve performance on imbalanced datasets.\n",
        "* **Threshold Moving:** Adjust the decision threshold if reducing false negatives is critical.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0_2CMSvGAL4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "Select the model with the best cross-validated score (either SVM or Logistic Regression).\n",
        "\n",
        "Evaluate it on the test set to measure real-world performance"
      ],
      "metadata": {
        "id": "ox918Tte_YoO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvEMD-CtJQWM"
      },
      "source": [
        "Search for the model that yields the highest F1-score. To do this, you should utilize the F1 micro score.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2aSearNJQWM"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation via grid search\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "model = LogisticRegression(max_iter=1, tol=1e-2,class_weight='balanced')  # balanced\n",
        "paramaters = [\n",
        "                {'C' : np.arange(0.001,1000,100), 'solver':['liblinear','saga']}\n",
        "\n",
        "             ]\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator = model,\n",
        "                           param_grid = paramaters,\n",
        "                           scoring = 'f1_micro',\n",
        "                           cv = 10,\n",
        "                           n_jobs = -1,\n",
        "                           verbose = 1)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_f1 = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best f1 : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_  )\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cohld7D0vKyy",
        "outputId": "a3af4b0c-97b7-46b9-fffd-3439db6e1668"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
            "Best f1 :  0.8325000000000001\n",
            "Best parameters : {'C': np.float64(0.001), 'solver': 'liblinear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      1195\n",
            "           1       0.79      0.98      0.88      1352\n",
            "           2       0.88      0.81      0.84      1157\n",
            "           3       0.83      0.84      0.83      1258\n",
            "           4       0.84      0.89      0.86      1140\n",
            "           5       0.84      0.70      0.76      1076\n",
            "           6       0.89      0.92      0.90      1167\n",
            "           7       0.85      0.86      0.85      1268\n",
            "           8       0.85      0.71      0.77      1174\n",
            "           9       0.81      0.76      0.79      1213\n",
            "\n",
            "    accuracy                           0.85     12000\n",
            "   macro avg       0.85      0.84      0.84     12000\n",
            "weighted avg       0.85      0.85      0.84     12000\n",
            "\n",
            "[[1146    0    9    3    5    6   19    0    5    2]\n",
            " [   0 1326    3    2    0    9    2    1    9    0]\n",
            " [  23   42  935   34   21    5   46   19   28    4]\n",
            " [   4   32   49 1053    1   25   16   37   20   21]\n",
            " [   1   28    9    1 1012   10   10    3    9   57]\n",
            " [  27   35    7   96   33  752   38    9   53   26]\n",
            " [  27   16   14    0   14   15 1071    0   10    0]\n",
            " [   8   41   13    9   25    3    0 1091    1   77]\n",
            " [  18  139   13   47   22   67    7    3  830   28]\n",
            " [  22   18   11   29   69    8    0  123    7  926]]\n",
            "84.51666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Breakdown: Cross-Validation via Grid Search for Logistic Regression**\n",
        "\n",
        "This code uses **GridSearchCV** to optimize a **Logistic Regression** model, focusing on maximizing the **F1-micro score**. This approach is well-suited for imbalanced datasets where precise classification of minority classes is critical.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "```\n",
        "\n",
        "* **GridSearchCV:** Performs an exhaustive search over a specified parameter grid, testing all possible combinations.\n",
        "* **LogisticRegression:** Implements logistic regression for binary or multiclass classification problems.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Initializing the Logistic Regression Model**\n",
        "\n",
        "```python\n",
        "model = LogisticRegression(max_iter=1, tol=1e-2, class_weight='balanced')\n",
        "```\n",
        "\n",
        "* **max\\_iter=1:** Sets a very low maximum number of iterations, which may lead to convergence warnings if the model is complex.\n",
        "* **tol=1e-2:** Sets the tolerance for stopping criteria. The model stops when the change in the loss function is smaller than this value.\n",
        "* **class\\_weight='balanced':** Automatically adjusts the weights of each class inversely proportional to their frequencies, addressing class imbalance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Defining the Hyperparameter Grid**\n",
        "\n",
        "```python\n",
        "paramaters = [\n",
        "    {\n",
        "        'C': np.arange(0.001, 1000, 100),\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "* **C (Regularization Parameter):**\n",
        "\n",
        "  * Controls the strength of regularization.\n",
        "  * **Low C:** Stronger regularization, simpler model, less overfitting.\n",
        "  * **High C:** Weaker regularization, complex model, more overfitting risk.\n",
        "* **solver:** The optimization algorithm used for training.\n",
        "\n",
        "  * **liblinear:** Suitable for small datasets, supports both L1 and L2 regularization.\n",
        "  * **saga:** Optimized for large datasets, also supports both L1 and L2 regularization, and is faster with sparse data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Setting Up the Grid Search**\n",
        "\n",
        "```python\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=paramaters,\n",
        "    scoring='f1_micro',\n",
        "    cv=10,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* **estimator:** The base model to optimize, in this case, **LogisticRegression**.\n",
        "* **param\\_grid:** The hyperparameter grid defined earlier.\n",
        "* **scoring='f1\\_micro':**\n",
        "\n",
        "  * Uses the **F1-micro** score, which calculates the harmonic mean of precision and recall.\n",
        "  * Suitable for multiclass problems where you care about overall precision and recall.\n",
        "* **cv=10:** 10-fold cross-validation, which means the data is split into 10 subsets, and the model is validated on each subset.\n",
        "* **n\\_jobs=-1:** Utilizes all available CPU cores for faster training.\n",
        "* **verbose=1:** Prints progress updates during training.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Fitting the Grid Search**\n",
        "\n",
        "```python\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Trains the logistic regression model using the provided training data.\n",
        "* Tests every possible combination of hyperparameters in the **param\\_grid** using 10-fold cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Extracting the Best Hyperparameters and Score**\n",
        "\n",
        "```python\n",
        "best_f1 = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best f1 : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_)\n",
        "```\n",
        "\n",
        "* **best\\_score\\_:** The highest cross-validated F1-micro score achieved.\n",
        "* **best\\_params\\_:** The set of hyperparameters that yielded the best F1-micro score.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Testing the Best Model on the Test Set**\n",
        "\n",
        "```python\n",
        "y_pred = grid_search.predict(X_test)\n",
        "```\n",
        "\n",
        "* Uses the best model found during the grid search to make predictions on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Evaluating the Test Set Performance**\n",
        "\n",
        "```python\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* **classification\\_report:** Provides precision, recall, F1-score, and support for each class.\n",
        "* It is a comprehensive metric for evaluating classification performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. Generating the Confusion Matrix**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:** Outputs a matrix showing the counts of true positives, true negatives, false positives, and false negatives.\n",
        "* Helps visualize the distribution of classification errors.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. Printing the Overall Test Accuracy**\n",
        "\n",
        "```python\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* **accuracy\\_score:** Provides the overall accuracy as a percentage, without accounting for class imbalance.\n",
        "* **F1-micro** score is usually more informative in this context, as it considers both precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations:**\n",
        "\n",
        "1. **F1-Micro Score Advantage:**\n",
        "\n",
        "   * Balances precision and recall, making it a better metric than plain accuracy for imbalanced datasets.\n",
        "\n",
        "2. **Balanced Class Weights:**\n",
        "\n",
        "   * Using **class\\_weight='balanced'** helps avoid bias toward the majority class.\n",
        "\n",
        "3. **Solver Selection:**\n",
        "\n",
        "   * The inclusion of **liblinear** and **saga** allows flexibility depending on the dataset size.\n",
        "\n",
        "4. **Potential Overfitting Risk:**\n",
        "\n",
        "   * Using a wide range of **C** values without regularization can lead to overfitting, especially with very high **C** values.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Tf53WTpDAdKk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHjHDPyru_Uw"
      },
      "source": [
        "Search for the model that yields the highest F1-score. To do this, you should utilize the F1 micro score.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S7843poJQWM"
      },
      "source": [
        "Since the dataset is slightly imbalanced, most trained models tend to have a 'preference' for the majority class. Investigate whether you can increase the F1-score (or recall) by applying the following techniques:\n",
        "\n",
        "Passing the parameter class_weight='balanced' to the models.\n",
        "Performing oversampling using SMOTE. More information: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html. You may need to install the imbalanced-learn library using pip3 install imbalanced-learn or in Anaconda: **conda install conda-forge::imbalanced-learn**\n",
        "\n",
        "\n",
        "Discuss the results you have achieved, paying particular attention to the F1 micro score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversampling with SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
        "\n",
        "print(y_resampled.shape)\n",
        "\n",
        "model = LogisticRegression(max_iter=1, tol=1e-2)\n",
        "paramaters = [\n",
        "                {'C' : np.arange(0.001,1000,100), 'solver':['liblinear','saga']}\n",
        "\n",
        "             ]\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator = model,\n",
        "                           param_grid = paramaters,\n",
        "                           scoring = 'f1_micro',\n",
        "                           cv = 10,\n",
        "                           n_jobs = -1,\n",
        "                           verbose = 1)\n",
        "grid_search = grid_search.fit(X_resampled, y_resampled)\n",
        "\n",
        "best_f1 = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best f1 : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_  )\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VynjQ6ku-Hs",
        "outputId": "12904b35-95dc-46e1-b536-ddf920ab9e84"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2370,)\n",
            "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
            "Best f1 :  0.8535864978902954\n",
            "Best parameters : {'C': np.float64(0.001), 'solver': 'liblinear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      1195\n",
            "           1       0.81      0.98      0.88      1352\n",
            "           2       0.89      0.77      0.83      1157\n",
            "           3       0.80      0.85      0.83      1258\n",
            "           4       0.85      0.89      0.87      1140\n",
            "           5       0.83      0.71      0.77      1076\n",
            "           6       0.90      0.91      0.90      1167\n",
            "           7       0.84      0.87      0.85      1268\n",
            "           8       0.84      0.72      0.78      1174\n",
            "           9       0.82      0.76      0.79      1213\n",
            "\n",
            "    accuracy                           0.85     12000\n",
            "   macro avg       0.85      0.84      0.84     12000\n",
            "weighted avg       0.85      0.85      0.84     12000\n",
            "\n",
            "[[1150    0    6    4    5    7   12    2    7    2]\n",
            " [   0 1322    3    3    0   11    2    1   10    0]\n",
            " [  23   43  895   61   23    5   46   17   40    4]\n",
            " [   4   28   37 1066    1   26   14   36   25   21]\n",
            " [   2   24   10    1 1011   12    7    4   10   59]\n",
            " [  28   29    5  103   31  765   35   13   45   22]\n",
            " [  27   18   16    0   13   19 1062    0   12    0]\n",
            " [   8   37   11   10   24    2    0 1103    2   71]\n",
            " [  16  125   11   50   21   68    5    3  849   26]\n",
            " [  23   14   10   28   67    9    0  134    5  923]]\n",
            "84.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Explanation: Logistic Regression with SMOTE Oversampling and Grid Search**\n",
        "\n",
        "This code block demonstrates how to improve the performance of a **Logistic Regression** model on an **imbalanced dataset** using **SMOTE** (Synthetic Minority Oversampling Technique) for oversampling the minority class, followed by a **Grid Search** for hyperparameter tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "```\n",
        "\n",
        "* **SMOTE:** Generates synthetic samples for the minority class to balance the dataset.\n",
        "* **GridSearchCV:** Performs an exhaustive search over a specified parameter grid for the best hyperparameters.\n",
        "* **LogisticRegression:** Implements the logistic regression model.\n",
        "* **classification\\_report, confusion\\_matrix, accuracy\\_score:** Tools for evaluating the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Oversampling with SMOTE**\n",
        "\n",
        "```python\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
        "print(y_resampled.shape)\n",
        "```\n",
        "\n",
        "* **SMOTE (Synthetic Minority Oversampling Technique):**\n",
        "\n",
        "  * Generates synthetic samples for the minority class.\n",
        "  * Helps mitigate the imbalance by creating new, plausible minority class examples.\n",
        "* **fit\\_resample:**\n",
        "\n",
        "  * Trains the SMOTE algorithm on the training data and then generates synthetic samples.\n",
        "* **Output:** The shape of the resampled data, indicating the increased size due to oversampling.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Model Initialization**\n",
        "\n",
        "```python\n",
        "model = LogisticRegression(max_iter=1, tol=1e-2)\n",
        "```\n",
        "\n",
        "* **max\\_iter=1:** Sets a very low maximum number of iterations, which may be too restrictive for complex models.\n",
        "* **tol=1e-2:** Sets the stopping criterion for the optimization algorithm. A larger tolerance means faster but potentially less accurate convergence.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Defining the Hyperparameter Grid**\n",
        "\n",
        "```python\n",
        "paramaters = [\n",
        "    {'C' : np.arange(0.001, 1000, 100), 'solver':['liblinear','saga']}    \n",
        "]\n",
        "```\n",
        "\n",
        "* **C (Regularization Strength):**\n",
        "\n",
        "  * Controls the regularization of the model.\n",
        "  * **Smaller C:** Stronger regularization, simpler models, less overfitting.\n",
        "  * **Larger C:** Weaker regularization, more complex models, higher risk of overfitting.\n",
        "* **solver:** Optimization algorithm for logistic regression.\n",
        "\n",
        "  * **liblinear:** Suitable for smaller datasets, supports both L1 and L2 penalties.\n",
        "  * **saga:** Faster on large datasets and supports both L1 and L2 penalties.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Setting Up the Grid Search**\n",
        "\n",
        "```python\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=paramaters,\n",
        "    scoring='f1_micro',\n",
        "    cv=10,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* **estimator:** The base logistic regression model.\n",
        "* **param\\_grid:** The hyperparameter grid defined earlier.\n",
        "* **scoring='f1\\_micro':**\n",
        "\n",
        "  * The F1-micro score calculates the overall precision and recall across all classes.\n",
        "  * It is well-suited for multiclass problems where you want to optimize the global performance.\n",
        "* **cv=10:** 10-fold cross-validation, meaning the data is split into 10 subsets for training and validation.\n",
        "* **n\\_jobs=-1:** Uses all available CPU cores for parallel processing, speeding up the training.\n",
        "* **verbose=1:** Prints progress messages during training, useful for tracking the search.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Training the Model**\n",
        "\n",
        "```python\n",
        "grid_search = grid_search.fit(X_resampled, y_resampled)\n",
        "```\n",
        "\n",
        "* Trains the logistic regression model using the oversampled data.\n",
        "* Tests every possible combination of hyperparameters in the **param\\_grid** using 10-fold cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Extracting the Best Hyperparameters and Score**\n",
        "\n",
        "```python\n",
        "best_f1 = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best f1 : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_)\n",
        "```\n",
        "\n",
        "* **best\\_score\\_:** The highest cross-validated F1-micro score achieved.\n",
        "* **best\\_params\\_:** The hyperparameter combination that yielded the best score.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Evaluating the Test Set Performance**\n",
        "\n",
        "```python\n",
        "y_pred = grid_search.predict(X_test)\n",
        "```\n",
        "\n",
        "* Uses the best model found during the grid search to make predictions on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. Detailed Classification Report**\n",
        "\n",
        "```python\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* Provides precision, recall, F1-score, and support for each class.\n",
        "* Offers a more comprehensive view of model performance than simple accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. Generating the Confusion Matrix**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:** Shows the distribution of true positives, true negatives, false positives, and false negatives.\n",
        "* Crucial for understanding where the model makes the most errors.\n",
        "\n",
        "---\n",
        "\n",
        "#### **11. Printing the Overall Test Accuracy**\n",
        "\n",
        "```python\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* **accuracy\\_score:** Provides the overall accuracy as a percentage, which can be misleading if the dataset is imbalanced.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations:**\n",
        "\n",
        "1. **SMOTE for Imbalanced Data:**\n",
        "\n",
        "   * Using SMOTE effectively balances the training data, improving the model's ability to detect minority classes.\n",
        "\n",
        "2. **High Bias Risk:**\n",
        "\n",
        "   * With **max\\_iter=1**, the model might not fully converge, potentially leading to underfitting.\n",
        "\n",
        "3. **Potential Overfitting:**\n",
        "\n",
        "   * Overfitting can still occur if the resampled data introduces too much noise.\n",
        "\n",
        "4. **Computational Efficiency:**\n",
        "\n",
        "   * Using **n\\_jobs=-1** and **saga** helps speed up the training on larger datasets.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w7FfucI6Atu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tAfY2LpvVrF",
        "outputId": "15a7b94b-d9e6-4b4e-dea9-a105778998c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwrnKaswJQWM"
      },
      "outputs": [],
      "source": [
        "# Balancing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f-eDtFeJQWM",
        "outputId": "07099c26-a4e1-4173-a17d-2bfd65309d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(516,)\n"
          ]
        }
      ],
      "source": [
        "# Oversampling with SMOTE\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X_resampled, y_resampled = SMOTE( ).fit_resample(X_train, y_train)\n",
        "\n",
        "print(y_resampled.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i91fH_QvJQWM"
      },
      "source": [
        "Suppose there is a demand to reduce the number of false negatives to 0. A false negative means that the diagnosis is benign while the tumor is actually malignant.\n",
        "\n",
        "\n",
        "Your options are to apply classweight balancing or threshold moving.\n",
        "\n",
        "\n",
        "Threshold moving, in the context of binary classification models like logistic regression or support vector machines, refers to adjusting the threshold value used to make predictions.\n",
        "\n",
        "By default, these models classify instances into one of two classes based on whether the predicted probability (output of the model) exceeds a certain threshold. For instance, if the threshold is set to 0.5, any instance with a predicted probability greater than 0.5 is classified as positive, while instances with predicted probabilities less than or equal to 0.5 are classified as negative.\n",
        "\n",
        "Threshold moving involves changing this threshold value to optimize specific metrics like accuracy, precision, recall, or F1-score. For example, if reducing false negatives is critical (e.g., in medical diagnosis where missing a positive case could be harmful), one might lower the threshold to ensure more instances are classified as positive, thus potentially reducing false negatives.\n",
        "\n",
        "\n",
        "\n",
        "Which threshold should be set to ensure that the model predicts no false negatives on the test set while still maintaining the highest possible accuracy? Plot the number of false negatives as a function of the threshold. Discuss the results."
      ]
    },
    {
      "source": [
        "# Reducing the number of false negatives\n",
        "y_pred_prob = grid_search.predict_proba(X_test)\n",
        "\n",
        "y_pred_class = y_pred_prob[:,1]\n",
        "\n",
        "false_negatives = []\n",
        "accuracy = []\n",
        "for threshold in np.linspace(0,1,100):\n",
        "    y_pred_class = y_pred_prob[:,1].copy()\n",
        "    #print(y_pred_poly_class)\n",
        "    y_pred_class[y_pred_class>=threshold]=1\n",
        "    y_pred_class[y_pred_class<threshold]=0\n",
        "    #print(y_pred_poly_class)\n",
        "    false_negatives.append(confusion_matrix(y_test,y_pred_class)[1,0])\n",
        "    accuracy.append(accuracy_score(y_test,y_pred_class)*100)\n",
        "false_negatives = np.asarray(false_negatives)\n",
        "#plt.bar(np.linspace(0,1,50),false_negatives)\n",
        "plt.figure(figsize=(15,5))\n",
        "chart = sns.barplot(x=np.round(np.linspace(0,1,100),2),y=false_negatives)\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=90,fontsize=7)\n",
        "chart.set_title('number of false negatives as a function of the threshold')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "chart = sns.barplot(x=np.round(np.linspace(0,1,100),2),y=accuracy)\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=90,fontsize=7)\n",
        "chart.set_title('accuracy as a function of the threshold')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "xEoJQLC4wLWR",
        "outputId": "aa2b3836-8e19-406f-a698-001803d511ed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-eb26f7ec5e96>:20: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  chart.set_xticklabels(chart.get_xticklabels(), rotation=90,fontsize=7)\n",
            "<ipython-input-44-eb26f7ec5e96>:26: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  chart.set_xticklabels(chart.get_xticklabels(), rotation=90,fontsize=7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'accuracy as a function of the threshold')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Explanation: Reducing the Number of False Negatives with Threshold Tuning**\n",
        "\n",
        "This code block attempts to reduce the number of **False Negatives (FN)** in a binary classification model by adjusting the decision threshold. This is particularly important in **medical diagnostics**, where missing a positive case (e.g., cancer) can be far more critical than a false alarm.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Getting the Predicted Probabilities**\n",
        "\n",
        "```python\n",
        "y_pred_prob = grid_search.predict_proba(X_test)\n",
        "```\n",
        "\n",
        "* **predict\\_proba:** Returns the predicted probabilities for each class.\n",
        "* **Output:** A 2D array where each row contains the probability of each class (e.g., \\[P(Class 0), P(Class 1)] for binary classification).\n",
        "* **Shape:** (n\\_samples, 2) for binary classification.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Extracting the Probability of the Positive Class (Malignant)**\n",
        "\n",
        "```python\n",
        "y_pred_class = y_pred_prob[:,1]\n",
        "```\n",
        "\n",
        "* **\\[:, 1]:** Selects the second column, which contains the probability of being in the **positive class** (malignant).\n",
        "* This is the critical value for threshold tuning, as we want to adjust the cutoff point for declaring a sample as malignant.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Initializing Storage for Results**\n",
        "\n",
        "```python\n",
        "false_negatives = []\n",
        "accuracy = []\n",
        "```\n",
        "\n",
        "* **false\\_negatives:** Will store the count of false negatives for each threshold.\n",
        "* **accuracy:** Will store the overall accuracy for each threshold.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Threshold Tuning Loop**\n",
        "\n",
        "```python\n",
        "for threshold in np.linspace(0,1,100):\n",
        "    y_pred_class = y_pred_prob[:,1].copy()\n",
        "    y_pred_class[y_pred_class >= threshold] = 1\n",
        "    y_pred_class[y_pred_class < threshold] = 0\n",
        "```\n",
        "\n",
        "* **np.linspace(0, 1, 100):** Generates 100 thresholds evenly spaced between 0 and 1.\n",
        "* **Threshold Logic:**\n",
        "\n",
        "  * **Threshold = 0:** Every sample is predicted as positive (no false negatives, but lots of false positives).\n",
        "  * **Threshold = 1:** Every sample is predicted as negative (no false positives, but lots of false negatives).\n",
        "* **Binary Decision:**\n",
        "\n",
        "  * Samples with a probability **greater than or equal to** the threshold are classified as positive (1).\n",
        "  * Samples **below** the threshold are classified as negative (0).\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Counting False Negatives and Calculating Accuracy**\n",
        "\n",
        "```python\n",
        "    false_negatives.append(confusion_matrix(y_test, y_pred_class)[1,0])\n",
        "    accuracy.append(accuracy_score(y_test, y_pred_class) * 100)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:**\n",
        "\n",
        "  * Index `[1, 0]` refers to **False Negatives** (actual **positive** but predicted **negative**).\n",
        "* **accuracy\\_score:**\n",
        "\n",
        "  * Computes the percentage of correctly classified samples.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Converting Lists to Numpy Arrays for Plotting**\n",
        "\n",
        "```python\n",
        "false_negatives = np.asarray(false_negatives)\n",
        "```\n",
        "\n",
        "* Converts the list of false negatives into a **numpy array** for easier plotting and processing.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Plotting False Negatives vs. Threshold**\n",
        "\n",
        "```python\n",
        "plt.figure(figsize=(15,5))\n",
        "chart = sns.barplot(x=np.round(np.linspace(0,1,100),2), y=false_negatives)\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=90, fontsize=7)\n",
        "chart.set_title('Number of False Negatives as a Function of the Threshold')\n",
        "```\n",
        "\n",
        "* **sns.barplot:**\n",
        "\n",
        "  * Creates a bar plot of **false negatives** as a function of **threshold**.\n",
        "* **np.round:**\n",
        "\n",
        "  * Rounds the threshold values for cleaner axis labels.\n",
        "* **rotation=90:** Rotates the x-axis labels for better readability.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Plotting Accuracy vs. Threshold**\n",
        "\n",
        "```python\n",
        "plt.figure(figsize=(15,5))\n",
        "chart = sns.barplot(x=np.round(np.linspace(0,1,100),2), y=accuracy)\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=90, fontsize=7)\n",
        "chart.set_title('Accuracy as a Function of the Threshold')\n",
        "```\n",
        "\n",
        "* Creates a second bar plot, this time for **accuracy** as a function of **threshold**.\n",
        "* This allows you to visually compare the trade-off between false negatives and overall accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Observations:**\n",
        "\n",
        "1. **Threshold 0.5 (Default):**\n",
        "\n",
        "   * Typically the default decision boundary, but might not be optimal for **imbalanced** datasets.\n",
        "\n",
        "2. **Lower Thresholds:**\n",
        "\n",
        "   * Reduce false negatives but at the cost of more false positives (lower precision).\n",
        "\n",
        "3. **Higher Thresholds:**\n",
        "\n",
        "   * Increase false negatives but improve precision (fewer false positives).\n",
        "\n",
        "4. **Critical Trade-Off:**\n",
        "\n",
        "   * In medical diagnostics, it's often better to have more **false positives** than **false negatives** to avoid missing dangerous cases.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UzIZwhbLA9aT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKASH0QEJQWM"
      },
      "source": [
        "## 2 - MNIST\n",
        "\n",
        "The MNIST (\"Modified National Institute of Standards and Technology\") dataset is commonly used for testing and benchmarking classification algorithms. It contains tens of thousands of images of handwritten digits. More information about this dataset can be found at: [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "For this task, three data files are provided:\n",
        "\n",
        "1. 'train.csv' contains labeled data for training the classifier.\n",
        "2. 'test.csv' contains labeled data for testing the trained classifier.\n",
        "3. 'test_Kaggle.csv' contains unlabeled data for evaluating the classifier through the competition found at [Kaggle Digit Recognizer](https://www.kaggle.com/c/digit-recognizer). This data is only useful for participating in the Kaggle competition.\n",
        "\n",
        "1) Train both a logistic regression classifier and a Support Vector Machine (with or without kernel). Tune the hyperparameters to find the most performant classifier by maximizing accuracy or minimizing the error rate on the 'test.csv' dataset. The error rate = 1 - accuracy. Training Support Vector Machines (especially those with kernel) requires significant computational resources. Therefore, it is advisable to initially train on a small portion of the training set. Training via logistic regression is less demanding on the CPU; nonetheless, it is recommended to use the lbfgs solver (LogisticRegression(multi_class='multinomial', solver='lbfgs')).\n",
        "\n",
        "2) Provide comments in the code and write down your conclusions and decisions.\n",
        "\n",
        "3) Is normalization necessary here? Which normalization method would you use? Is StandardScaler a good choice?\n",
        "\n",
        "4) Investigate the two different types of multiclass classification: one-vs-one (ovo) or one-vs-rest (ovr). Focus on accuracy and computation time. What are the conclusions?\n",
        "\n",
        "5) Test your final classifier with some self-written digits. What are the findings? What does classification accuracy depend on?\n",
        "\n",
        "6) Optional: Test on the 'test_Kaggle' dataset and upload the results in the correct format to the Kaggle website. What score did you achieve? Compare this score with the score on [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "You can visualize a digit using 'plt.imshow(X_train[n].reshape((28, 28)),cmap = 'gray')'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d98wCwxJJQWN",
        "outputId": "0ff2ccfa-469a-4bfb-d604-89930f3b9687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
              "0      1       0       0       0       0       0       0       0       0   \n",
              "1      0       0       0       0       0       0       0       0       0   \n",
              "2      1       0       0       0       0       0       0       0       0   \n",
              "3      4       0       0       0       0       0       0       0       0   \n",
              "4      0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel8  pixel9  pixel10  pixel11  pixel12  pixel13  pixel14  pixel15  \\\n",
              "0       0       0        0        0        0        0        0        0   \n",
              "1       0       0        0        0        0        0        0        0   \n",
              "2       0       0        0        0        0        0        0        0   \n",
              "3       0       0        0        0        0        0        0        0   \n",
              "4       0       0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel16  pixel17  pixel18  pixel19  pixel20  pixel21  pixel22  pixel23  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel24  pixel25  pixel26  pixel27  pixel28  pixel29  pixel30  pixel31  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel32  pixel33  pixel34  pixel35  pixel36  pixel37  pixel38  pixel39  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel40  pixel41  pixel42  pixel43  pixel44  pixel45  pixel46  pixel47  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel48  pixel49  pixel50  pixel51  pixel52  pixel53  pixel54  pixel55  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel56  pixel57  pixel58  pixel59  pixel60  pixel61  pixel62  pixel63  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel64  pixel65  pixel66  pixel67  pixel68  pixel69  pixel70  pixel71  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel72  pixel73  pixel74  pixel75  pixel76  pixel77  pixel78  pixel79  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel80  pixel81  pixel82  pixel83  pixel84  pixel85  pixel86  pixel87  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel88  pixel89  pixel90  pixel91  pixel92  pixel93  pixel94  pixel95  \\\n",
              "0        0        0        0        0        0        0        0        0   \n",
              "1        0        0        0        0        0        0        0        0   \n",
              "2        0        0        0        0        0        0        0        0   \n",
              "3        0        0        0        0        0        0        0        0   \n",
              "4        0        0        0        0        0        0        0        0   \n",
              "\n",
              "   pixel96  pixel97  pixel98  pixel99  pixel100  pixel101  pixel102  pixel103  \\\n",
              "0        0        0        0        0         0         0         0         0   \n",
              "1        0        0        0        0         0         0         0         0   \n",
              "2        0        0        0        0         0         0         0         0   \n",
              "3        0        0        0        0         0         0         0         0   \n",
              "4        0        0        0        0         0         0         0         0   \n",
              "\n",
              "   pixel104  pixel105  pixel106  pixel107  pixel108  pixel109  pixel110  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel111  pixel112  pixel113  pixel114  pixel115  pixel116  pixel117  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel118  pixel119  pixel120  pixel121  pixel122  pixel123  pixel124  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0        18        30       137   \n",
              "2         0         0         0         0         0         0         3   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         1        25       130       155   \n",
              "\n",
              "   pixel125  pixel126  pixel127  pixel128  pixel129  pixel130  pixel131  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       137       192        86        72         1         0         0   \n",
              "2       141       139         3         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       254       254       254       157        30         2         0   \n",
              "\n",
              "   pixel132  pixel133  pixel134  pixel135  pixel136  pixel137  pixel138  \\\n",
              "0       188       255        94         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel139  pixel140  pixel141  pixel142  pixel143  pixel144  pixel145  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel146  pixel147  pixel148  pixel149  pixel150  pixel151  pixel152  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0        13        86       250       254       254   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3       220       179         6         0         0         0         0   \n",
              "4         0         0         8       103       253       253       253   \n",
              "\n",
              "   pixel153  pixel154  pixel155  pixel156  pixel157  pixel158  pixel159  \\\n",
              "0         0         0         0         0         0         0       191   \n",
              "1       254       254       217       246       151        32         0   \n",
              "2       254       254         8         0         0         0         0   \n",
              "3         0         0         0         0         9        77         0   \n",
              "4       253       253       253       253       253       114         2   \n",
              "\n",
              "   pixel160  pixel161  pixel162  pixel163  pixel164  pixel165  pixel166  \\\n",
              "0       250       253        93         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel167  pixel168  pixel169  pixel170  pixel171  pixel172  pixel173  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel174  pixel175  pixel176  pixel177  pixel178  pixel179  pixel180  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0        16       179       254       254       254       254   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3        28       247        17         0         0         0         0   \n",
              "4         0        11       208       253       253       253       253   \n",
              "\n",
              "   pixel181  pixel182  pixel183  pixel184  pixel185  pixel186  pixel187  \\\n",
              "0         0         0         0         0         0       123       248   \n",
              "1       254       254       254       254       254       231        54   \n",
              "2       254       254         8         0         0         0         0   \n",
              "3         0         0         0         0        27       202         0   \n",
              "4       253       253       253       253       253       253       107   \n",
              "\n",
              "   pixel188  pixel189  pixel190  pixel191  pixel192  pixel193  pixel194  \\\n",
              "0       253       167        10         0         0         0         0   \n",
              "1        15         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel195  pixel196  pixel197  pixel198  pixel199  pixel200  pixel201  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel202  pixel203  pixel204  pixel205  pixel206  pixel207  pixel208  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0        72       254       254       254       254       254   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0       242       155         0         0         0         0   \n",
              "4         0        31       253       253       253       253       253   \n",
              "\n",
              "   pixel209  pixel210  pixel211  pixel212  pixel213  pixel214  pixel215  \\\n",
              "0         0         0         0         0        80       247       253   \n",
              "1       254       254       254       254       254       254       254   \n",
              "2       254       254       106         0         0         0         0   \n",
              "3         0         0         0         0        27       254        63   \n",
              "4       253       253       253       253       253       253       215   \n",
              "\n",
              "   pixel216  pixel217  pixel218  pixel219  pixel220  pixel221  pixel222  \\\n",
              "0       208        13         0         0         0         0         0   \n",
              "1       104         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       101         3         0         0         0         0         0   \n",
              "\n",
              "   pixel223  pixel224  pixel225  pixel226  pixel227  pixel228  pixel229  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel230  pixel231  pixel232  pixel233  pixel234  pixel235  pixel236  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1        61       191       254       254       254       254       254   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0       160       207         6         0         0         0   \n",
              "4        23       210       253       253       253       248       161   \n",
              "\n",
              "   pixel237  pixel238  pixel239  pixel240  pixel241  pixel242  pixel243  \\\n",
              "0         0         0         0        29       207       253       235   \n",
              "1       109        83       199       254       254       254       254   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0        27       254        65   \n",
              "4       222       222       246       253       253       253       253   \n",
              "\n",
              "   pixel244  pixel245  pixel246  pixel247  pixel248  pixel249  pixel250  \\\n",
              "0        77         0         0         0         0         0         0   \n",
              "1       243        85         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       253        39         0         0         0         0         0   \n",
              "\n",
              "   pixel251  pixel252  pixel253  pixel254  pixel255  pixel256  pixel257  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel258  pixel259  pixel260  pixel261  pixel262  pixel263  pixel264  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       172       254       254       254       202       147       147   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0       127       254        21         0         0         0   \n",
              "4       136       253       253       253       229        77         0   \n",
              "\n",
              "   pixel265  pixel266  pixel267  pixel268  pixel269  pixel270  pixel271  \\\n",
              "0         0         0        54       209       253       253        88   \n",
              "1        45         0        11        29       200       254       254   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0        20       239        65   \n",
              "4         0         0        70       218       253       253       253   \n",
              "\n",
              "   pixel272  pixel273  pixel274  pixel275  pixel276  pixel277  pixel278  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       171         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       253       215        91         0         0         0         0   \n",
              "\n",
              "   pixel279  pixel280  pixel281  pixel282  pixel283  pixel284  pixel285  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         1   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         5   \n",
              "\n",
              "   pixel286  pixel287  pixel288  pixel289  pixel290  pixel291  pixel292  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       174       254       254        89        67         0         0   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0        77       254        21         0         0         0   \n",
              "4       214       253       253       253       195         0         0   \n",
              "\n",
              "   pixel293  pixel294  pixel295  pixel296  pixel297  pixel298  pixel299  \\\n",
              "0         0        93       254       253       238       170        17   \n",
              "1         0         0         0         0       128       252       254   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0       195        65   \n",
              "4         0         0         0       104       224       253       253   \n",
              "\n",
              "   pixel300  pixel301  pixel302  pixel303  pixel304  pixel305  pixel306  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       212        76         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       253       253       215        29         0         0         0   \n",
              "\n",
              "   pixel307  pixel308  pixel309  pixel310  pixel311  pixel312  pixel313  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0        47   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       116   \n",
              "\n",
              "   pixel314  pixel315  pixel316  pixel317  pixel318  pixel319  pixel320  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       254        29         0         0         0   \n",
              "2         0         0         0         0         0         0         6   \n",
              "3         0        70       254        21         0         0         0   \n",
              "4       253       253       253       247        75         0         0   \n",
              "\n",
              "   pixel321  pixel322  pixel323  pixel324  pixel325  pixel326  pixel327  \\\n",
              "0        23       210       254       253       159         0         0   \n",
              "1         0         0         0         0         0        83       254   \n",
              "2       185       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0       195       142   \n",
              "4         0         0         0         0        26       200       253   \n",
              "\n",
              "   pixel328  pixel329  pixel330  pixel331  pixel332  pixel333  pixel334  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       153         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       253       253       253       216         4         0         0   \n",
              "\n",
              "   pixel335  pixel336  pixel337  pixel338  pixel339  pixel340  pixel341  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0        80   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel342  pixel343  pixel344  pixel345  pixel346  pixel347  pixel348  \\\n",
              "0         0         0         0         0         0         0        16   \n",
              "1       254       254       240        24         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0        56       251        21         0         0         0   \n",
              "4       253       253       253       195         0         0         0   \n",
              "\n",
              "   pixel349  pixel350  pixel351  pixel352  pixel353  pixel354  pixel355  \\\n",
              "0       209       253       254       240        81         0         0   \n",
              "1         0         0         0         0         0        25       240   \n",
              "2        89       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0       195       227   \n",
              "4         0         0         0         0         0        26       200   \n",
              "\n",
              "   pixel356  pixel357  pixel358  pixel359  pixel360  pixel361  pixel362  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       153         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       253       253       253       253         5         0         0   \n",
              "\n",
              "   pixel363  pixel364  pixel365  pixel366  pixel367  pixel368  pixel369  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0        64   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel370  pixel371  pixel372  pixel373  pixel374  pixel375  pixel376  \\\n",
              "0         0         0         0         0         0         0        27   \n",
              "1       254       254       186         7         0         0         0   \n",
              "2         0         0         0         0         0         0         4   \n",
              "3         0         0       222       153         5         0         0   \n",
              "4       253       253       253        99         0         0         0   \n",
              "\n",
              "   pixel377  pixel378  pixel379  pixel380  pixel381  pixel382  pixel383  \\\n",
              "0       253       253       254        13         0         0         0   \n",
              "1         0         0         0         0         0         0       166   \n",
              "2       146       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0       120       240   \n",
              "4         0         0         0         0         0         0        25   \n",
              "\n",
              "   pixel384  pixel385  pixel386  pixel387  pixel388  pixel389  pixel390  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       224        12         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3        13         0         0         0         0         0         0   \n",
              "4       231       253       253       253        36         0         0   \n",
              "\n",
              "   pixel391  pixel392  pixel393  pixel394  pixel395  pixel396  pixel397  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0        14       232   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel398  pixel399  pixel400  pixel401  pixel402  pixel403  pixel404  \\\n",
              "0         0         0         0         0         0        20       206   \n",
              "1       254       254       254        29         0         0         0   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0         0        67       251        40         0         0   \n",
              "4       253       253       253        99         0         0         0   \n",
              "\n",
              "   pixel405  pixel406  pixel407  pixel408  pixel409  pixel410  pixel411  \\\n",
              "0       254       254       198         7         0         0         0   \n",
              "1         0         0         0         0         0         0        75   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0        94       255   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel412  pixel413  pixel414  pixel415  pixel416  pixel417  pixel418  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       254        17         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3        69         0         0         0         0         0         0   \n",
              "4       223       253       253       253       129         0         0   \n",
              "\n",
              "   pixel419  pixel420  pixel421  pixel422  pixel423  pixel424  pixel425  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0        18       254   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel426  pixel427  pixel428  pixel429  pixel430  pixel431  pixel432  \\\n",
              "0         0         0         0         0         0       168       253   \n",
              "1       254       254       254        29         0         0         0   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0         0         0       234       184         0         0   \n",
              "4       253       253       253        99         0         0         0   \n",
              "\n",
              "   pixel433  pixel434  pixel435  pixel436  pixel437  pixel438  pixel439  \\\n",
              "0       253       196         7         0         0         0         0   \n",
              "1         0         0         0         0         0         0        48   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0        19       245   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel440  pixel441  pixel442  pixel443  pixel444  pixel445  pixel446  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       254        17         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3        69         0         0         0         0         0         0   \n",
              "4       127       253       253       253       129         0         0   \n",
              "\n",
              "   pixel447  pixel448  pixel449  pixel450  pixel451  pixel452  pixel453  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         2       163   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel454  pixel455  pixel456  pixel457  pixel458  pixel459  pixel460  \\\n",
              "0         0         0         0         0        20       203       253   \n",
              "1       254       254       254        29         0         0         0   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0         0         0       234       169         0         0   \n",
              "4       253       253       253        99         0         0         0   \n",
              "\n",
              "   pixel461  pixel462  pixel463  pixel464  pixel465  pixel466  pixel467  \\\n",
              "0       248        76         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0        48   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0         3       199   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel468  pixel469  pixel470  pixel471  pixel472  pixel473  pixel474  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       254        17         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       182        10         0         0         0         0         0   \n",
              "4       139       253       253       253        90         0         0   \n",
              "\n",
              "   pixel475  pixel476  pixel477  pixel478  pixel479  pixel480  pixel481  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0        94   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel482  pixel483  pixel484  pixel485  pixel486  pixel487  pixel488  \\\n",
              "0         0         0         0        22       188       253       245   \n",
              "1       254       254       254       200        12         0         0   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0         0         0       154       205         4         0   \n",
              "4       253       253       253        99         0         0         0   \n",
              "\n",
              "   pixel489  pixel490  pixel491  pixel492  pixel493  pixel494  pixel495  \\\n",
              "0        93         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0        16       209   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0        26        72       128       203       208       254   \n",
              "4         0         0         0         0         0         0        78   \n",
              "\n",
              "   pixel496  pixel497  pixel498  pixel499  pixel500  pixel501  pixel502  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       150         1         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       254       131         0         0         0         0         0   \n",
              "4       248       253       253       253         5         0         0   \n",
              "\n",
              "   pixel503  pixel504  pixel505  pixel506  pixel507  pixel508  pixel509  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0        15   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       254   \n",
              "\n",
              "   pixel510  pixel511  pixel512  pixel513  pixel514  pixel515  pixel516  \\\n",
              "0         0         0         0       103       253       253       191   \n",
              "1       206       254       254       254       202        66         0   \n",
              "2         0         0         0         0         0         0         9   \n",
              "3         0         0         0        61       254       129       113   \n",
              "4       253       253       253       216        34         0         0   \n",
              "\n",
              "   pixel517  pixel518  pixel519  pixel520  pixel521  pixel522  pixel523  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0        21       161       254   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3       186       245       251       189        75        56       136   \n",
              "4         0         0         0         0         0        33       152   \n",
              "\n",
              "   pixel524  pixel525  pixel526  pixel527  pixel528  pixel529  pixel530  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       245        31         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       254        73         0         0         0         0         0   \n",
              "4       253       253       253       107         1         0         0   \n",
              "\n",
              "   pixel531  pixel532  pixel533  pixel534  pixel535  pixel536  pixel537  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0       206   \n",
              "\n",
              "   pixel538  pixel539  pixel540  pixel541  pixel542  pixel543  pixel544  \\\n",
              "0         0         0        89       240       253       195        25   \n",
              "1        60       212       254       254       254       194        48   \n",
              "2         0         0         0         0         0         0       156   \n",
              "3         0         0         0        15       216       233       233   \n",
              "4       253       253       253       253       140         0         0   \n",
              "\n",
              "   pixel545  pixel546  pixel547  pixel548  pixel549  pixel550  pixel551  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1        48        34        41        48       209       254       254   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3       159       104        52         0         0         0        38   \n",
              "4         0         0         0        30       139       234       253   \n",
              "\n",
              "   pixel552  pixel553  pixel554  pixel555  pixel556  pixel557  pixel558  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       171         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       254        73         0         0         0         0         0   \n",
              "4       253       253       154         2         0         0         0   \n",
              "\n",
              "   pixel559  pixel560  pixel561  pixel562  pixel563  pixel564  pixel565  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0        16   \n",
              "\n",
              "   pixel566  pixel567  pixel568  pixel569  pixel570  pixel571  pixel572  \\\n",
              "0         0        15       220       253       253        80         0   \n",
              "1         0        86       243       254       254       254       254   \n",
              "2         0         0         0         0         0         0       185   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       205       253       253       253       250       208       106   \n",
              "\n",
              "   pixel573  pixel574  pixel575  pixel576  pixel577  pixel578  pixel579  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       233       243       254       254       254       254   \n",
              "2       255       255       184         0         0         0         0   \n",
              "3         0         0         0         0         0         0        18   \n",
              "4       106       106       200       237       253       253       253   \n",
              "\n",
              "   pixel580  pixel581  pixel582  pixel583  pixel584  pixel585  pixel586  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254        86         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       254        73         0         0         0         0         0   \n",
              "4       253       209        22         0         0         0         0   \n",
              "\n",
              "   pixel587  pixel588  pixel589  pixel590  pixel591  pixel592  pixel593  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel594  pixel595  pixel596  pixel597  pixel598  pixel599  pixel600  \\\n",
              "0         0        94       253       253       253        94         0   \n",
              "1         0         0       114       254       254       254       254   \n",
              "2         0         0         0         0         0         0       185   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4        82       253       253       253       253       253       253   \n",
              "\n",
              "   pixel601  pixel602  pixel603  pixel604  pixel605  pixel606  pixel607  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       254       254       254       254       239   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0         0        18   \n",
              "4       253       253       253       253       253       253       253   \n",
              "\n",
              "   pixel608  pixel609  pixel610  pixel611  pixel612  pixel613  pixel614  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1        86        11         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       254        73         0         0         0         0         0   \n",
              "4       209        22         0         0         0         0         0   \n",
              "\n",
              "   pixel615  pixel616  pixel617  pixel618  pixel619  pixel620  pixel621  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel622  pixel623  pixel624  pixel625  pixel626  pixel627  pixel628  \\\n",
              "0         0        89       251       253       250       131         0   \n",
              "1         0         0        13       182       254       254       254   \n",
              "2         0         0         0         0         0         0       185   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         1        91       253       253       253       253       253   \n",
              "\n",
              "   pixel629  pixel630  pixel631  pixel632  pixel633  pixel634  pixel635  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       254       254       254       254       254       243        70   \n",
              "2       254       254       184         0         0         0         0   \n",
              "3         0         0         0         0         0         0         5   \n",
              "4       253       253       253       253       253       213        90   \n",
              "\n",
              "   pixel636  pixel637  pixel638  pixel639  pixel640  pixel641  pixel642  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       206       106         0         0         0         0         0   \n",
              "4         7         0         0         0         0         0         0   \n",
              "\n",
              "   pixel643  pixel644  pixel645  pixel646  pixel647  pixel648  pixel649  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel650  pixel651  pixel652  pixel653  pixel654  pixel655  pixel656  \\\n",
              "0         0         0       214       218        95         0         0   \n",
              "1         0         0         0         8        76       146       254   \n",
              "2         0         0         0         0         0         0        63   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         1        18       129       208       253       253   \n",
              "\n",
              "   pixel657  pixel658  pixel659  pixel660  pixel661  pixel662  pixel663  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1       255       254       255       146        19        15         0   \n",
              "2       254       254        62         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4       253       253       159       129        90         4         0   \n",
              "\n",
              "   pixel664  pixel665  pixel666  pixel667  pixel668  pixel669  pixel670  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       186       159         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel671  pixel672  pixel673  pixel674  pixel675  pixel676  pixel677  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel678  pixel679  pixel680  pixel681  pixel682  pixel683  pixel684  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel685  pixel686  pixel687  pixel688  pixel689  pixel690  pixel691  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         6   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel692  pixel693  pixel694  pixel695  pixel696  pixel697  pixel698  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3       209       101         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel699  pixel700  pixel701  pixel702  pixel703  pixel704  pixel705  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel706  pixel707  pixel708  pixel709  pixel710  pixel711  pixel712  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel713  pixel714  pixel715  pixel716  pixel717  pixel718  pixel719  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel720  pixel721  pixel722  pixel723  pixel724  pixel725  pixel726  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel727  pixel728  pixel729  pixel730  pixel731  pixel732  pixel733  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel734  pixel735  pixel736  pixel737  pixel738  pixel739  pixel740  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel741  pixel742  pixel743  pixel744  pixel745  pixel746  pixel747  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel748  pixel749  pixel750  pixel751  pixel752  pixel753  pixel754  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel755  pixel756  pixel757  pixel758  pixel759  pixel760  pixel761  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel762  pixel763  pixel764  pixel765  pixel766  pixel767  pixel768  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel769  pixel770  pixel771  pixel772  pixel773  pixel774  pixel775  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  pixel782  \\\n",
              "0         0         0         0         0         0         0         0   \n",
              "1         0         0         0         0         0         0         0   \n",
              "2         0         0         0         0         0         0         0   \n",
              "3         0         0         0         0         0         0         0   \n",
              "4         0         0         0         0         0         0         0   \n",
              "\n",
              "   pixel783  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-007c21d3-369e-4f33-90ca-25aa6febcac8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>pixel40</th>\n",
              "      <th>pixel41</th>\n",
              "      <th>pixel42</th>\n",
              "      <th>pixel43</th>\n",
              "      <th>pixel44</th>\n",
              "      <th>pixel45</th>\n",
              "      <th>pixel46</th>\n",
              "      <th>pixel47</th>\n",
              "      <th>pixel48</th>\n",
              "      <th>pixel49</th>\n",
              "      <th>pixel50</th>\n",
              "      <th>pixel51</th>\n",
              "      <th>pixel52</th>\n",
              "      <th>pixel53</th>\n",
              "      <th>pixel54</th>\n",
              "      <th>pixel55</th>\n",
              "      <th>pixel56</th>\n",
              "      <th>pixel57</th>\n",
              "      <th>pixel58</th>\n",
              "      <th>pixel59</th>\n",
              "      <th>pixel60</th>\n",
              "      <th>pixel61</th>\n",
              "      <th>pixel62</th>\n",
              "      <th>pixel63</th>\n",
              "      <th>pixel64</th>\n",
              "      <th>pixel65</th>\n",
              "      <th>pixel66</th>\n",
              "      <th>pixel67</th>\n",
              "      <th>pixel68</th>\n",
              "      <th>pixel69</th>\n",
              "      <th>pixel70</th>\n",
              "      <th>pixel71</th>\n",
              "      <th>pixel72</th>\n",
              "      <th>pixel73</th>\n",
              "      <th>pixel74</th>\n",
              "      <th>pixel75</th>\n",
              "      <th>pixel76</th>\n",
              "      <th>pixel77</th>\n",
              "      <th>pixel78</th>\n",
              "      <th>pixel79</th>\n",
              "      <th>pixel80</th>\n",
              "      <th>pixel81</th>\n",
              "      <th>pixel82</th>\n",
              "      <th>pixel83</th>\n",
              "      <th>pixel84</th>\n",
              "      <th>pixel85</th>\n",
              "      <th>pixel86</th>\n",
              "      <th>pixel87</th>\n",
              "      <th>pixel88</th>\n",
              "      <th>pixel89</th>\n",
              "      <th>pixel90</th>\n",
              "      <th>pixel91</th>\n",
              "      <th>pixel92</th>\n",
              "      <th>pixel93</th>\n",
              "      <th>pixel94</th>\n",
              "      <th>pixel95</th>\n",
              "      <th>pixel96</th>\n",
              "      <th>pixel97</th>\n",
              "      <th>pixel98</th>\n",
              "      <th>pixel99</th>\n",
              "      <th>pixel100</th>\n",
              "      <th>pixel101</th>\n",
              "      <th>pixel102</th>\n",
              "      <th>pixel103</th>\n",
              "      <th>pixel104</th>\n",
              "      <th>pixel105</th>\n",
              "      <th>pixel106</th>\n",
              "      <th>pixel107</th>\n",
              "      <th>pixel108</th>\n",
              "      <th>pixel109</th>\n",
              "      <th>pixel110</th>\n",
              "      <th>pixel111</th>\n",
              "      <th>pixel112</th>\n",
              "      <th>pixel113</th>\n",
              "      <th>pixel114</th>\n",
              "      <th>pixel115</th>\n",
              "      <th>pixel116</th>\n",
              "      <th>pixel117</th>\n",
              "      <th>pixel118</th>\n",
              "      <th>pixel119</th>\n",
              "      <th>pixel120</th>\n",
              "      <th>pixel121</th>\n",
              "      <th>pixel122</th>\n",
              "      <th>pixel123</th>\n",
              "      <th>pixel124</th>\n",
              "      <th>pixel125</th>\n",
              "      <th>pixel126</th>\n",
              "      <th>pixel127</th>\n",
              "      <th>pixel128</th>\n",
              "      <th>pixel129</th>\n",
              "      <th>pixel130</th>\n",
              "      <th>pixel131</th>\n",
              "      <th>pixel132</th>\n",
              "      <th>pixel133</th>\n",
              "      <th>pixel134</th>\n",
              "      <th>pixel135</th>\n",
              "      <th>pixel136</th>\n",
              "      <th>pixel137</th>\n",
              "      <th>pixel138</th>\n",
              "      <th>pixel139</th>\n",
              "      <th>pixel140</th>\n",
              "      <th>pixel141</th>\n",
              "      <th>pixel142</th>\n",
              "      <th>pixel143</th>\n",
              "      <th>pixel144</th>\n",
              "      <th>pixel145</th>\n",
              "      <th>pixel146</th>\n",
              "      <th>pixel147</th>\n",
              "      <th>pixel148</th>\n",
              "      <th>pixel149</th>\n",
              "      <th>pixel150</th>\n",
              "      <th>pixel151</th>\n",
              "      <th>pixel152</th>\n",
              "      <th>pixel153</th>\n",
              "      <th>pixel154</th>\n",
              "      <th>pixel155</th>\n",
              "      <th>pixel156</th>\n",
              "      <th>pixel157</th>\n",
              "      <th>pixel158</th>\n",
              "      <th>pixel159</th>\n",
              "      <th>pixel160</th>\n",
              "      <th>pixel161</th>\n",
              "      <th>pixel162</th>\n",
              "      <th>pixel163</th>\n",
              "      <th>pixel164</th>\n",
              "      <th>pixel165</th>\n",
              "      <th>pixel166</th>\n",
              "      <th>pixel167</th>\n",
              "      <th>pixel168</th>\n",
              "      <th>pixel169</th>\n",
              "      <th>pixel170</th>\n",
              "      <th>pixel171</th>\n",
              "      <th>pixel172</th>\n",
              "      <th>pixel173</th>\n",
              "      <th>pixel174</th>\n",
              "      <th>pixel175</th>\n",
              "      <th>pixel176</th>\n",
              "      <th>pixel177</th>\n",
              "      <th>pixel178</th>\n",
              "      <th>pixel179</th>\n",
              "      <th>pixel180</th>\n",
              "      <th>pixel181</th>\n",
              "      <th>pixel182</th>\n",
              "      <th>pixel183</th>\n",
              "      <th>pixel184</th>\n",
              "      <th>pixel185</th>\n",
              "      <th>pixel186</th>\n",
              "      <th>pixel187</th>\n",
              "      <th>pixel188</th>\n",
              "      <th>pixel189</th>\n",
              "      <th>pixel190</th>\n",
              "      <th>pixel191</th>\n",
              "      <th>pixel192</th>\n",
              "      <th>pixel193</th>\n",
              "      <th>pixel194</th>\n",
              "      <th>pixel195</th>\n",
              "      <th>pixel196</th>\n",
              "      <th>pixel197</th>\n",
              "      <th>pixel198</th>\n",
              "      <th>pixel199</th>\n",
              "      <th>pixel200</th>\n",
              "      <th>pixel201</th>\n",
              "      <th>pixel202</th>\n",
              "      <th>pixel203</th>\n",
              "      <th>pixel204</th>\n",
              "      <th>pixel205</th>\n",
              "      <th>pixel206</th>\n",
              "      <th>pixel207</th>\n",
              "      <th>pixel208</th>\n",
              "      <th>pixel209</th>\n",
              "      <th>pixel210</th>\n",
              "      <th>pixel211</th>\n",
              "      <th>pixel212</th>\n",
              "      <th>pixel213</th>\n",
              "      <th>pixel214</th>\n",
              "      <th>pixel215</th>\n",
              "      <th>pixel216</th>\n",
              "      <th>pixel217</th>\n",
              "      <th>pixel218</th>\n",
              "      <th>pixel219</th>\n",
              "      <th>pixel220</th>\n",
              "      <th>pixel221</th>\n",
              "      <th>pixel222</th>\n",
              "      <th>pixel223</th>\n",
              "      <th>pixel224</th>\n",
              "      <th>pixel225</th>\n",
              "      <th>pixel226</th>\n",
              "      <th>pixel227</th>\n",
              "      <th>pixel228</th>\n",
              "      <th>pixel229</th>\n",
              "      <th>pixel230</th>\n",
              "      <th>pixel231</th>\n",
              "      <th>pixel232</th>\n",
              "      <th>pixel233</th>\n",
              "      <th>pixel234</th>\n",
              "      <th>pixel235</th>\n",
              "      <th>pixel236</th>\n",
              "      <th>pixel237</th>\n",
              "      <th>pixel238</th>\n",
              "      <th>pixel239</th>\n",
              "      <th>pixel240</th>\n",
              "      <th>pixel241</th>\n",
              "      <th>pixel242</th>\n",
              "      <th>pixel243</th>\n",
              "      <th>pixel244</th>\n",
              "      <th>pixel245</th>\n",
              "      <th>pixel246</th>\n",
              "      <th>pixel247</th>\n",
              "      <th>pixel248</th>\n",
              "      <th>pixel249</th>\n",
              "      <th>pixel250</th>\n",
              "      <th>pixel251</th>\n",
              "      <th>pixel252</th>\n",
              "      <th>pixel253</th>\n",
              "      <th>pixel254</th>\n",
              "      <th>pixel255</th>\n",
              "      <th>pixel256</th>\n",
              "      <th>pixel257</th>\n",
              "      <th>pixel258</th>\n",
              "      <th>pixel259</th>\n",
              "      <th>pixel260</th>\n",
              "      <th>pixel261</th>\n",
              "      <th>pixel262</th>\n",
              "      <th>pixel263</th>\n",
              "      <th>pixel264</th>\n",
              "      <th>pixel265</th>\n",
              "      <th>pixel266</th>\n",
              "      <th>pixel267</th>\n",
              "      <th>pixel268</th>\n",
              "      <th>pixel269</th>\n",
              "      <th>pixel270</th>\n",
              "      <th>pixel271</th>\n",
              "      <th>pixel272</th>\n",
              "      <th>pixel273</th>\n",
              "      <th>pixel274</th>\n",
              "      <th>pixel275</th>\n",
              "      <th>pixel276</th>\n",
              "      <th>pixel277</th>\n",
              "      <th>pixel278</th>\n",
              "      <th>pixel279</th>\n",
              "      <th>pixel280</th>\n",
              "      <th>pixel281</th>\n",
              "      <th>pixel282</th>\n",
              "      <th>pixel283</th>\n",
              "      <th>pixel284</th>\n",
              "      <th>pixel285</th>\n",
              "      <th>pixel286</th>\n",
              "      <th>pixel287</th>\n",
              "      <th>pixel288</th>\n",
              "      <th>pixel289</th>\n",
              "      <th>pixel290</th>\n",
              "      <th>pixel291</th>\n",
              "      <th>pixel292</th>\n",
              "      <th>pixel293</th>\n",
              "      <th>pixel294</th>\n",
              "      <th>pixel295</th>\n",
              "      <th>pixel296</th>\n",
              "      <th>pixel297</th>\n",
              "      <th>pixel298</th>\n",
              "      <th>pixel299</th>\n",
              "      <th>pixel300</th>\n",
              "      <th>pixel301</th>\n",
              "      <th>pixel302</th>\n",
              "      <th>pixel303</th>\n",
              "      <th>pixel304</th>\n",
              "      <th>pixel305</th>\n",
              "      <th>pixel306</th>\n",
              "      <th>pixel307</th>\n",
              "      <th>pixel308</th>\n",
              "      <th>pixel309</th>\n",
              "      <th>pixel310</th>\n",
              "      <th>pixel311</th>\n",
              "      <th>pixel312</th>\n",
              "      <th>pixel313</th>\n",
              "      <th>pixel314</th>\n",
              "      <th>pixel315</th>\n",
              "      <th>pixel316</th>\n",
              "      <th>pixel317</th>\n",
              "      <th>pixel318</th>\n",
              "      <th>pixel319</th>\n",
              "      <th>pixel320</th>\n",
              "      <th>pixel321</th>\n",
              "      <th>pixel322</th>\n",
              "      <th>pixel323</th>\n",
              "      <th>pixel324</th>\n",
              "      <th>pixel325</th>\n",
              "      <th>pixel326</th>\n",
              "      <th>pixel327</th>\n",
              "      <th>pixel328</th>\n",
              "      <th>pixel329</th>\n",
              "      <th>pixel330</th>\n",
              "      <th>pixel331</th>\n",
              "      <th>pixel332</th>\n",
              "      <th>pixel333</th>\n",
              "      <th>pixel334</th>\n",
              "      <th>pixel335</th>\n",
              "      <th>pixel336</th>\n",
              "      <th>pixel337</th>\n",
              "      <th>pixel338</th>\n",
              "      <th>pixel339</th>\n",
              "      <th>pixel340</th>\n",
              "      <th>pixel341</th>\n",
              "      <th>pixel342</th>\n",
              "      <th>pixel343</th>\n",
              "      <th>pixel344</th>\n",
              "      <th>pixel345</th>\n",
              "      <th>pixel346</th>\n",
              "      <th>pixel347</th>\n",
              "      <th>pixel348</th>\n",
              "      <th>pixel349</th>\n",
              "      <th>pixel350</th>\n",
              "      <th>pixel351</th>\n",
              "      <th>pixel352</th>\n",
              "      <th>pixel353</th>\n",
              "      <th>pixel354</th>\n",
              "      <th>pixel355</th>\n",
              "      <th>pixel356</th>\n",
              "      <th>pixel357</th>\n",
              "      <th>pixel358</th>\n",
              "      <th>pixel359</th>\n",
              "      <th>pixel360</th>\n",
              "      <th>pixel361</th>\n",
              "      <th>pixel362</th>\n",
              "      <th>pixel363</th>\n",
              "      <th>pixel364</th>\n",
              "      <th>pixel365</th>\n",
              "      <th>pixel366</th>\n",
              "      <th>pixel367</th>\n",
              "      <th>pixel368</th>\n",
              "      <th>pixel369</th>\n",
              "      <th>pixel370</th>\n",
              "      <th>pixel371</th>\n",
              "      <th>pixel372</th>\n",
              "      <th>pixel373</th>\n",
              "      <th>pixel374</th>\n",
              "      <th>pixel375</th>\n",
              "      <th>pixel376</th>\n",
              "      <th>pixel377</th>\n",
              "      <th>pixel378</th>\n",
              "      <th>pixel379</th>\n",
              "      <th>pixel380</th>\n",
              "      <th>pixel381</th>\n",
              "      <th>pixel382</th>\n",
              "      <th>pixel383</th>\n",
              "      <th>pixel384</th>\n",
              "      <th>pixel385</th>\n",
              "      <th>pixel386</th>\n",
              "      <th>pixel387</th>\n",
              "      <th>pixel388</th>\n",
              "      <th>pixel389</th>\n",
              "      <th>pixel390</th>\n",
              "      <th>pixel391</th>\n",
              "      <th>pixel392</th>\n",
              "      <th>pixel393</th>\n",
              "      <th>pixel394</th>\n",
              "      <th>pixel395</th>\n",
              "      <th>pixel396</th>\n",
              "      <th>pixel397</th>\n",
              "      <th>pixel398</th>\n",
              "      <th>pixel399</th>\n",
              "      <th>pixel400</th>\n",
              "      <th>pixel401</th>\n",
              "      <th>pixel402</th>\n",
              "      <th>pixel403</th>\n",
              "      <th>pixel404</th>\n",
              "      <th>pixel405</th>\n",
              "      <th>pixel406</th>\n",
              "      <th>pixel407</th>\n",
              "      <th>pixel408</th>\n",
              "      <th>pixel409</th>\n",
              "      <th>pixel410</th>\n",
              "      <th>pixel411</th>\n",
              "      <th>pixel412</th>\n",
              "      <th>pixel413</th>\n",
              "      <th>pixel414</th>\n",
              "      <th>pixel415</th>\n",
              "      <th>pixel416</th>\n",
              "      <th>pixel417</th>\n",
              "      <th>pixel418</th>\n",
              "      <th>pixel419</th>\n",
              "      <th>pixel420</th>\n",
              "      <th>pixel421</th>\n",
              "      <th>pixel422</th>\n",
              "      <th>pixel423</th>\n",
              "      <th>pixel424</th>\n",
              "      <th>pixel425</th>\n",
              "      <th>pixel426</th>\n",
              "      <th>pixel427</th>\n",
              "      <th>pixel428</th>\n",
              "      <th>pixel429</th>\n",
              "      <th>pixel430</th>\n",
              "      <th>pixel431</th>\n",
              "      <th>pixel432</th>\n",
              "      <th>pixel433</th>\n",
              "      <th>pixel434</th>\n",
              "      <th>pixel435</th>\n",
              "      <th>pixel436</th>\n",
              "      <th>pixel437</th>\n",
              "      <th>pixel438</th>\n",
              "      <th>pixel439</th>\n",
              "      <th>pixel440</th>\n",
              "      <th>pixel441</th>\n",
              "      <th>pixel442</th>\n",
              "      <th>pixel443</th>\n",
              "      <th>pixel444</th>\n",
              "      <th>pixel445</th>\n",
              "      <th>pixel446</th>\n",
              "      <th>pixel447</th>\n",
              "      <th>pixel448</th>\n",
              "      <th>pixel449</th>\n",
              "      <th>pixel450</th>\n",
              "      <th>pixel451</th>\n",
              "      <th>pixel452</th>\n",
              "      <th>pixel453</th>\n",
              "      <th>pixel454</th>\n",
              "      <th>pixel455</th>\n",
              "      <th>pixel456</th>\n",
              "      <th>pixel457</th>\n",
              "      <th>pixel458</th>\n",
              "      <th>pixel459</th>\n",
              "      <th>pixel460</th>\n",
              "      <th>pixel461</th>\n",
              "      <th>pixel462</th>\n",
              "      <th>pixel463</th>\n",
              "      <th>pixel464</th>\n",
              "      <th>pixel465</th>\n",
              "      <th>pixel466</th>\n",
              "      <th>pixel467</th>\n",
              "      <th>pixel468</th>\n",
              "      <th>pixel469</th>\n",
              "      <th>pixel470</th>\n",
              "      <th>pixel471</th>\n",
              "      <th>pixel472</th>\n",
              "      <th>pixel473</th>\n",
              "      <th>pixel474</th>\n",
              "      <th>pixel475</th>\n",
              "      <th>pixel476</th>\n",
              "      <th>pixel477</th>\n",
              "      <th>pixel478</th>\n",
              "      <th>pixel479</th>\n",
              "      <th>pixel480</th>\n",
              "      <th>pixel481</th>\n",
              "      <th>pixel482</th>\n",
              "      <th>pixel483</th>\n",
              "      <th>pixel484</th>\n",
              "      <th>pixel485</th>\n",
              "      <th>pixel486</th>\n",
              "      <th>pixel487</th>\n",
              "      <th>pixel488</th>\n",
              "      <th>pixel489</th>\n",
              "      <th>pixel490</th>\n",
              "      <th>pixel491</th>\n",
              "      <th>pixel492</th>\n",
              "      <th>pixel493</th>\n",
              "      <th>pixel494</th>\n",
              "      <th>pixel495</th>\n",
              "      <th>pixel496</th>\n",
              "      <th>pixel497</th>\n",
              "      <th>pixel498</th>\n",
              "      <th>pixel499</th>\n",
              "      <th>pixel500</th>\n",
              "      <th>pixel501</th>\n",
              "      <th>pixel502</th>\n",
              "      <th>pixel503</th>\n",
              "      <th>pixel504</th>\n",
              "      <th>pixel505</th>\n",
              "      <th>pixel506</th>\n",
              "      <th>pixel507</th>\n",
              "      <th>pixel508</th>\n",
              "      <th>pixel509</th>\n",
              "      <th>pixel510</th>\n",
              "      <th>pixel511</th>\n",
              "      <th>pixel512</th>\n",
              "      <th>pixel513</th>\n",
              "      <th>pixel514</th>\n",
              "      <th>pixel515</th>\n",
              "      <th>pixel516</th>\n",
              "      <th>pixel517</th>\n",
              "      <th>pixel518</th>\n",
              "      <th>pixel519</th>\n",
              "      <th>pixel520</th>\n",
              "      <th>pixel521</th>\n",
              "      <th>pixel522</th>\n",
              "      <th>pixel523</th>\n",
              "      <th>pixel524</th>\n",
              "      <th>pixel525</th>\n",
              "      <th>pixel526</th>\n",
              "      <th>pixel527</th>\n",
              "      <th>pixel528</th>\n",
              "      <th>pixel529</th>\n",
              "      <th>pixel530</th>\n",
              "      <th>pixel531</th>\n",
              "      <th>pixel532</th>\n",
              "      <th>pixel533</th>\n",
              "      <th>pixel534</th>\n",
              "      <th>pixel535</th>\n",
              "      <th>pixel536</th>\n",
              "      <th>pixel537</th>\n",
              "      <th>pixel538</th>\n",
              "      <th>pixel539</th>\n",
              "      <th>pixel540</th>\n",
              "      <th>pixel541</th>\n",
              "      <th>pixel542</th>\n",
              "      <th>pixel543</th>\n",
              "      <th>pixel544</th>\n",
              "      <th>pixel545</th>\n",
              "      <th>pixel546</th>\n",
              "      <th>pixel547</th>\n",
              "      <th>pixel548</th>\n",
              "      <th>pixel549</th>\n",
              "      <th>pixel550</th>\n",
              "      <th>pixel551</th>\n",
              "      <th>pixel552</th>\n",
              "      <th>pixel553</th>\n",
              "      <th>pixel554</th>\n",
              "      <th>pixel555</th>\n",
              "      <th>pixel556</th>\n",
              "      <th>pixel557</th>\n",
              "      <th>pixel558</th>\n",
              "      <th>pixel559</th>\n",
              "      <th>pixel560</th>\n",
              "      <th>pixel561</th>\n",
              "      <th>pixel562</th>\n",
              "      <th>pixel563</th>\n",
              "      <th>pixel564</th>\n",
              "      <th>pixel565</th>\n",
              "      <th>pixel566</th>\n",
              "      <th>pixel567</th>\n",
              "      <th>pixel568</th>\n",
              "      <th>pixel569</th>\n",
              "      <th>pixel570</th>\n",
              "      <th>pixel571</th>\n",
              "      <th>pixel572</th>\n",
              "      <th>pixel573</th>\n",
              "      <th>pixel574</th>\n",
              "      <th>pixel575</th>\n",
              "      <th>pixel576</th>\n",
              "      <th>pixel577</th>\n",
              "      <th>pixel578</th>\n",
              "      <th>pixel579</th>\n",
              "      <th>pixel580</th>\n",
              "      <th>pixel581</th>\n",
              "      <th>pixel582</th>\n",
              "      <th>pixel583</th>\n",
              "      <th>pixel584</th>\n",
              "      <th>pixel585</th>\n",
              "      <th>pixel586</th>\n",
              "      <th>pixel587</th>\n",
              "      <th>pixel588</th>\n",
              "      <th>pixel589</th>\n",
              "      <th>pixel590</th>\n",
              "      <th>pixel591</th>\n",
              "      <th>pixel592</th>\n",
              "      <th>pixel593</th>\n",
              "      <th>pixel594</th>\n",
              "      <th>pixel595</th>\n",
              "      <th>pixel596</th>\n",
              "      <th>pixel597</th>\n",
              "      <th>pixel598</th>\n",
              "      <th>pixel599</th>\n",
              "      <th>pixel600</th>\n",
              "      <th>pixel601</th>\n",
              "      <th>pixel602</th>\n",
              "      <th>pixel603</th>\n",
              "      <th>pixel604</th>\n",
              "      <th>pixel605</th>\n",
              "      <th>pixel606</th>\n",
              "      <th>pixel607</th>\n",
              "      <th>pixel608</th>\n",
              "      <th>pixel609</th>\n",
              "      <th>pixel610</th>\n",
              "      <th>pixel611</th>\n",
              "      <th>pixel612</th>\n",
              "      <th>pixel613</th>\n",
              "      <th>pixel614</th>\n",
              "      <th>pixel615</th>\n",
              "      <th>pixel616</th>\n",
              "      <th>pixel617</th>\n",
              "      <th>pixel618</th>\n",
              "      <th>pixel619</th>\n",
              "      <th>pixel620</th>\n",
              "      <th>pixel621</th>\n",
              "      <th>pixel622</th>\n",
              "      <th>pixel623</th>\n",
              "      <th>pixel624</th>\n",
              "      <th>pixel625</th>\n",
              "      <th>pixel626</th>\n",
              "      <th>pixel627</th>\n",
              "      <th>pixel628</th>\n",
              "      <th>pixel629</th>\n",
              "      <th>pixel630</th>\n",
              "      <th>pixel631</th>\n",
              "      <th>pixel632</th>\n",
              "      <th>pixel633</th>\n",
              "      <th>pixel634</th>\n",
              "      <th>pixel635</th>\n",
              "      <th>pixel636</th>\n",
              "      <th>pixel637</th>\n",
              "      <th>pixel638</th>\n",
              "      <th>pixel639</th>\n",
              "      <th>pixel640</th>\n",
              "      <th>pixel641</th>\n",
              "      <th>pixel642</th>\n",
              "      <th>pixel643</th>\n",
              "      <th>pixel644</th>\n",
              "      <th>pixel645</th>\n",
              "      <th>pixel646</th>\n",
              "      <th>pixel647</th>\n",
              "      <th>pixel648</th>\n",
              "      <th>pixel649</th>\n",
              "      <th>pixel650</th>\n",
              "      <th>pixel651</th>\n",
              "      <th>pixel652</th>\n",
              "      <th>pixel653</th>\n",
              "      <th>pixel654</th>\n",
              "      <th>pixel655</th>\n",
              "      <th>pixel656</th>\n",
              "      <th>pixel657</th>\n",
              "      <th>pixel658</th>\n",
              "      <th>pixel659</th>\n",
              "      <th>pixel660</th>\n",
              "      <th>pixel661</th>\n",
              "      <th>pixel662</th>\n",
              "      <th>pixel663</th>\n",
              "      <th>pixel664</th>\n",
              "      <th>pixel665</th>\n",
              "      <th>pixel666</th>\n",
              "      <th>pixel667</th>\n",
              "      <th>pixel668</th>\n",
              "      <th>pixel669</th>\n",
              "      <th>pixel670</th>\n",
              "      <th>pixel671</th>\n",
              "      <th>pixel672</th>\n",
              "      <th>pixel673</th>\n",
              "      <th>pixel674</th>\n",
              "      <th>pixel675</th>\n",
              "      <th>pixel676</th>\n",
              "      <th>pixel677</th>\n",
              "      <th>pixel678</th>\n",
              "      <th>pixel679</th>\n",
              "      <th>pixel680</th>\n",
              "      <th>pixel681</th>\n",
              "      <th>pixel682</th>\n",
              "      <th>pixel683</th>\n",
              "      <th>pixel684</th>\n",
              "      <th>pixel685</th>\n",
              "      <th>pixel686</th>\n",
              "      <th>pixel687</th>\n",
              "      <th>pixel688</th>\n",
              "      <th>pixel689</th>\n",
              "      <th>pixel690</th>\n",
              "      <th>pixel691</th>\n",
              "      <th>pixel692</th>\n",
              "      <th>pixel693</th>\n",
              "      <th>pixel694</th>\n",
              "      <th>pixel695</th>\n",
              "      <th>pixel696</th>\n",
              "      <th>pixel697</th>\n",
              "      <th>pixel698</th>\n",
              "      <th>pixel699</th>\n",
              "      <th>pixel700</th>\n",
              "      <th>pixel701</th>\n",
              "      <th>pixel702</th>\n",
              "      <th>pixel703</th>\n",
              "      <th>pixel704</th>\n",
              "      <th>pixel705</th>\n",
              "      <th>pixel706</th>\n",
              "      <th>pixel707</th>\n",
              "      <th>pixel708</th>\n",
              "      <th>pixel709</th>\n",
              "      <th>pixel710</th>\n",
              "      <th>pixel711</th>\n",
              "      <th>pixel712</th>\n",
              "      <th>pixel713</th>\n",
              "      <th>pixel714</th>\n",
              "      <th>pixel715</th>\n",
              "      <th>pixel716</th>\n",
              "      <th>pixel717</th>\n",
              "      <th>pixel718</th>\n",
              "      <th>pixel719</th>\n",
              "      <th>pixel720</th>\n",
              "      <th>pixel721</th>\n",
              "      <th>pixel722</th>\n",
              "      <th>pixel723</th>\n",
              "      <th>pixel724</th>\n",
              "      <th>pixel725</th>\n",
              "      <th>pixel726</th>\n",
              "      <th>pixel727</th>\n",
              "      <th>pixel728</th>\n",
              "      <th>pixel729</th>\n",
              "      <th>pixel730</th>\n",
              "      <th>pixel731</th>\n",
              "      <th>pixel732</th>\n",
              "      <th>pixel733</th>\n",
              "      <th>pixel734</th>\n",
              "      <th>pixel735</th>\n",
              "      <th>pixel736</th>\n",
              "      <th>pixel737</th>\n",
              "      <th>pixel738</th>\n",
              "      <th>pixel739</th>\n",
              "      <th>pixel740</th>\n",
              "      <th>pixel741</th>\n",
              "      <th>pixel742</th>\n",
              "      <th>pixel743</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>188</td>\n",
              "      <td>255</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>191</td>\n",
              "      <td>250</td>\n",
              "      <td>253</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>248</td>\n",
              "      <td>253</td>\n",
              "      <td>167</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>247</td>\n",
              "      <td>253</td>\n",
              "      <td>208</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>207</td>\n",
              "      <td>253</td>\n",
              "      <td>235</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>209</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>238</td>\n",
              "      <td>170</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>210</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>209</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>240</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>206</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>198</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>168</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>196</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>203</td>\n",
              "      <td>253</td>\n",
              "      <td>248</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>188</td>\n",
              "      <td>253</td>\n",
              "      <td>245</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>103</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>191</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>240</td>\n",
              "      <td>253</td>\n",
              "      <td>195</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>220</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>250</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>214</td>\n",
              "      <td>218</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>30</td>\n",
              "      <td>137</td>\n",
              "      <td>137</td>\n",
              "      <td>192</td>\n",
              "      <td>86</td>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>86</td>\n",
              "      <td>250</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>217</td>\n",
              "      <td>246</td>\n",
              "      <td>151</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>179</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>231</td>\n",
              "      <td>54</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>191</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>109</td>\n",
              "      <td>83</td>\n",
              "      <td>199</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>243</td>\n",
              "      <td>85</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>202</td>\n",
              "      <td>147</td>\n",
              "      <td>147</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>29</td>\n",
              "      <td>200</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>174</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>89</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>128</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>212</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>47</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>83</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>240</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>240</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>186</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>166</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>224</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>232</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>75</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>163</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>200</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>209</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>150</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>206</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>202</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>161</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>245</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>212</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>194</td>\n",
              "      <td>48</td>\n",
              "      <td>48</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>48</td>\n",
              "      <td>209</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>243</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>233</td>\n",
              "      <td>243</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>114</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>239</td>\n",
              "      <td>86</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>182</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>243</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>76</td>\n",
              "      <td>146</td>\n",
              "      <td>254</td>\n",
              "      <td>255</td>\n",
              "      <td>254</td>\n",
              "      <td>255</td>\n",
              "      <td>146</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>141</td>\n",
              "      <td>139</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>185</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>146</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>185</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>185</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>185</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>220</td>\n",
              "      <td>179</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>247</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>202</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>242</td>\n",
              "      <td>155</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>254</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>160</td>\n",
              "      <td>207</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>254</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>127</td>\n",
              "      <td>254</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>239</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>254</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "      <td>254</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>142</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>251</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>227</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>222</td>\n",
              "      <td>153</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>240</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>251</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>255</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>234</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>245</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>234</td>\n",
              "      <td>169</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>199</td>\n",
              "      <td>182</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>154</td>\n",
              "      <td>205</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>72</td>\n",
              "      <td>128</td>\n",
              "      <td>203</td>\n",
              "      <td>208</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>254</td>\n",
              "      <td>129</td>\n",
              "      <td>113</td>\n",
              "      <td>186</td>\n",
              "      <td>245</td>\n",
              "      <td>251</td>\n",
              "      <td>189</td>\n",
              "      <td>75</td>\n",
              "      <td>56</td>\n",
              "      <td>136</td>\n",
              "      <td>254</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>216</td>\n",
              "      <td>233</td>\n",
              "      <td>233</td>\n",
              "      <td>159</td>\n",
              "      <td>104</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>254</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>254</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>254</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>206</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>186</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>209</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>130</td>\n",
              "      <td>155</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>157</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>103</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>114</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>208</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>107</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>215</td>\n",
              "      <td>101</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>210</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>248</td>\n",
              "      <td>161</td>\n",
              "      <td>222</td>\n",
              "      <td>222</td>\n",
              "      <td>246</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>136</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>229</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "      <td>218</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>215</td>\n",
              "      <td>91</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>214</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>195</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>104</td>\n",
              "      <td>224</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>215</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>116</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>247</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>200</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>216</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>195</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>200</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>231</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>223</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>127</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>78</td>\n",
              "      <td>248</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>216</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>152</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>107</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>206</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>139</td>\n",
              "      <td>234</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>154</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>205</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>250</td>\n",
              "      <td>208</td>\n",
              "      <td>106</td>\n",
              "      <td>106</td>\n",
              "      <td>106</td>\n",
              "      <td>200</td>\n",
              "      <td>237</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>209</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>209</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>91</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>213</td>\n",
              "      <td>90</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>129</td>\n",
              "      <td>208</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>159</td>\n",
              "      <td>129</td>\n",
              "      <td>90</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-007c21d3-369e-4f33-90ca-25aa6febcac8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-007c21d3-369e-4f33-90ca-25aa6febcac8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-007c21d3-369e-4f33-90ca-25aa6febcac8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-807cf324-e205-4c97-8286-2c60021bb172\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-807cf324-e205-4c97-8286-2c60021bb172')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-807cf324-e205-4c97-8286-2c60021bb172 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Reading the dataset\n",
        "\n",
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3UDfiAipJQWN"
      },
      "outputs": [],
      "source": [
        "##### Classificatie van MNIST\n",
        "\n",
        "# Split into features and targets\n",
        "y_train = df_train['label']\n",
        "X_train = df_train.drop('label', axis=1)\n",
        "\n",
        "y_test = df_test['label']\n",
        "X_test = df_test.drop('label', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_train = np.asarray(y_train)\n",
        "y_test = np.asarray(y_test)\n",
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)\n",
        "\n",
        "# To reduce the computation time, you can limit the number of training samples. For example to 2000 digits.\n",
        "\n",
        "\n",
        "training_size = 2000\n",
        "\n",
        "X_train = X_train[0:training_size,:]\n",
        "y_train = y_train[0:training_size]\n",
        "\n",
        "\n",
        "# Scaling\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Explanation: MNIST Classification - Data Preparation and Scaling**\n",
        "\n",
        "This code block is part of the **MNIST (Modified National Institute of Standards and Technology)** digit classification task, where the goal is to train machine learning models to recognize handwritten digits (0-9) from pixel values.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Splitting Features and Targets**\n",
        "\n",
        "```python\n",
        "y_train = df_train['label']\n",
        "X_train = df_train.drop('label', axis=1)\n",
        "\n",
        "y_test = df_test['label']\n",
        "X_test = df_test.drop('label', axis=1)\n",
        "```\n",
        "\n",
        "* **df\\_train** and **df\\_test** are pandas DataFrames containing the training and testing data, respectively.\n",
        "* **Labels (Targets):** The **label** column contains the digit labels (0-9) and is separated from the features.\n",
        "* **Features:** All other columns represent the **784** pixel values (28x28 grayscale image).\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Converting to Numpy Arrays**\n",
        "\n",
        "```python\n",
        "y_train = np.asarray(y_train)\n",
        "y_test = np.asarray(y_test)\n",
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)\n",
        "```\n",
        "\n",
        "* **Why Convert?**\n",
        "\n",
        "  * Most machine learning libraries like **scikit-learn** expect input data to be in **numpy** arrays for efficient matrix operations.\n",
        "* **Shape:**\n",
        "\n",
        "  * **X\\_train:** (n\\_samples, 784)\n",
        "  * **y\\_train:** (n\\_samples,)\n",
        "* This conversion is critical for faster computations and compatibility with sklearn models.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Optional - Limiting the Training Set Size**\n",
        "\n",
        "```python\n",
        "training_size = 2000\n",
        "\n",
        "X_train = X_train[0:training_size, :]\n",
        "y_train = y_train[0:training_size]\n",
        "```\n",
        "\n",
        "* **Purpose:** To **reduce training time** when experimenting with models, especially with SVMs, which are computationally expensive.\n",
        "* **Effect:** Limits the training data to the first **2000** samples.\n",
        "* **Impact:** Faster training but potentially lower accuracy, as the model sees fewer examples.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Data Type Conversion for Efficient Computation**\n",
        "\n",
        "```python\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "```\n",
        "\n",
        "* **Why Use float32?**\n",
        "\n",
        "  * Reduces memory usage compared to the default **float64**.\n",
        "  * Speeds up matrix operations, which is important for large datasets like MNIST.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Scaling the Features**\n",
        "\n",
        "```python\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "```\n",
        "\n",
        "* **Why Scale?**\n",
        "\n",
        "  * Pixel values in MNIST range from **0** to **255**.\n",
        "  * Scaling to **\\[0, 1]** improves the convergence speed of optimization algorithms like **Logistic Regression** and **SVM**.\n",
        "* **Normalization Effect:**\n",
        "\n",
        "  * Removes the influence of pixel intensity, allowing the model to focus on the **shape** and **structure** of the digits rather than absolute brightness.\n",
        "* **Without Scaling:** The model would struggle to learn effectively, as it would consider pixel brightness more important than actual shape.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOC_mxdRBVNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation via random search\n",
        "\n",
        "\n",
        "model = LogisticRegression(max_iter=1, tol=1e-2)\n",
        "parameters = {'C': uniform(0.01, 100), 'solver':['liblinear','saga']}\n",
        "\n",
        "\n",
        "n_iter_search = 1\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
        "\n",
        "random_search = random_search.fit(X_train, y_train)\n",
        "\n",
        "best_accuracy = random_search.best_score_\n",
        "best_parameters = random_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', random_search.best_score_)\n",
        "print('Best parameters :',random_search.best_params_  )\n",
        "\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w4QlF6-kx2q",
        "outputId": "fd2d6052-35c6-4e67-ac7e-e8ee0a5c8b2b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Best accuracy :  0.817\n",
            "Best parameters : {'C': np.float64(26.905370279481623), 'solver': 'liblinear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92      1195\n",
            "           1       0.80      0.97      0.88      1352\n",
            "           2       0.82      0.80      0.81      1157\n",
            "           3       0.84      0.77      0.80      1258\n",
            "           4       0.81      0.87      0.84      1140\n",
            "           5       0.81      0.71      0.75      1076\n",
            "           6       0.89      0.89      0.89      1167\n",
            "           7       0.83      0.80      0.82      1268\n",
            "           8       0.83      0.71      0.77      1174\n",
            "           9       0.76      0.77      0.77      1213\n",
            "\n",
            "    accuracy                           0.83     12000\n",
            "   macro avg       0.83      0.83      0.82     12000\n",
            "weighted avg       0.83      0.83      0.83     12000\n",
            "\n",
            "[[1146    3   11    4    2    8   13    1    6    1]\n",
            " [   0 1318    6    0    5   11    1    3    8    0]\n",
            " [  25   37  924   51   17    4   53   17   24    5]\n",
            " [   7   30   79  968    6   46   13   45   28   36]\n",
            " [   7   29   14    3  995    8    7    6   12   59]\n",
            " [  31   24    8   71   37  760   32   13   73   27]\n",
            " [  32   18   23    0   26   18 1044    0    6    0]\n",
            " [  12   37   23    8   40    7    1 1019    2  119]\n",
            " [  22  129   16   30   19   73    7    4  833   41]\n",
            " [  16   13   19   23   75    8    0  118   10  931]]\n",
            "82.81666666666668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and testing an SVM model. Be aware that this can take a lot of time to compute\n",
        "model = SVC()\n",
        "parameters = {'kernel': ['linear','rbf','poly'],\n",
        "              'C': uniform(0.01, 1000), # haal C uit een random uniform distribution\n",
        "              'gamma': uniform(0.001, 1)}\n",
        "\n",
        "\n",
        "n_iter_search = 1\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
        "\n",
        "random_search = random_search.fit(X_train, y_train)\n",
        "\n",
        "best_accuracy = random_search.best_score_\n",
        "best_parameters = random_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', random_search.best_score_)\n",
        "print('Best parameters :',random_search.best_params_  )\n",
        "\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8gB2061lJCz",
        "outputId": "8e656ea7-5960-49dd-f68d-dc67aab4ebad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Best accuracy :  0.11850000000000001\n",
            "Best parameters : {'C': np.float64(449.41218323249444), 'gamma': np.float64(0.5726703548640166), 'kernel': 'rbf'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1195\n",
            "           1       0.00      0.00      0.00      1352\n",
            "           2       0.10      1.00      0.18      1157\n",
            "           3       0.00      0.00      0.00      1258\n",
            "           4       0.00      0.00      0.00      1140\n",
            "           5       0.00      0.00      0.00      1076\n",
            "           6       0.00      0.00      0.00      1167\n",
            "           7       0.00      0.00      0.00      1268\n",
            "           8       0.00      0.00      0.00      1174\n",
            "           9       0.00      0.00      0.00      1213\n",
            "\n",
            "    accuracy                           0.10     12000\n",
            "   macro avg       0.01      0.10      0.02     12000\n",
            "weighted avg       0.01      0.10      0.02     12000\n",
            "\n",
            "[[   0    0 1195    0    0    0    0    0    0    0]\n",
            " [   0    0 1352    0    0    0    0    0    0    0]\n",
            " [   0    0 1157    0    0    0    0    0    0    0]\n",
            " [   0    0 1258    0    0    0    0    0    0    0]\n",
            " [   0    0 1140    0    0    0    0    0    0    0]\n",
            " [   0    0 1076    0    0    0    0    0    0    0]\n",
            " [   0    0 1167    0    0    0    0    0    0    0]\n",
            " [   0    0 1268    0    0    0    0    0    0    0]\n",
            " [   0    0 1174    0    0    0    0    0    0    0]\n",
            " [   0    0 1213    0    0    0    0    0    0    0]]\n",
            "9.641666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Explanation: SVM Training and Hyperparameter Tuning using RandomizedSearchCV**\n",
        "\n",
        "This code cell is for training a **Support Vector Machine (SVM)** model on the MNIST dataset with hyperparameter tuning using **RandomizedSearchCV**. SVMs are powerful classifiers, but they can be computationally intensive, especially with large datasets like MNIST.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Model Initialization**\n",
        "\n",
        "```python\n",
        "model = SVC()\n",
        "```\n",
        "\n",
        "* **SVC (Support Vector Classifier)** is used for classification tasks.\n",
        "* **SVC** supports different types of kernels like **linear**, **rbf** (Radial Basis Function), and **polynomial**.\n",
        "* By default, **SVC** uses an RBF kernel.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Defining the Hyperparameter Space**\n",
        "\n",
        "```python\n",
        "parameters = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': uniform(0.01, 1000),  # Regularization parameter\n",
        "    'gamma': uniform(0.001, 1) # Kernel coefficient (only for rbf and poly)\n",
        "}\n",
        "```\n",
        "\n",
        "* **Kernel:** Specifies the type of decision boundary. The options are:\n",
        "\n",
        "  * **linear:** Straight-line decision boundary.\n",
        "  * **rbf (Radial Basis Function):** Non-linear boundary that can handle complex decision surfaces.\n",
        "  * **poly:** Polynomial decision boundary.\n",
        "* **C (Regularization Parameter):**\n",
        "\n",
        "  * Controls the trade-off between maximizing the margin and minimizing classification error.\n",
        "  * Higher **C** values = lower bias, higher variance (more complex model).\n",
        "  * Lower **C** values = higher bias, lower variance (simpler model).\n",
        "* **Gamma:**\n",
        "\n",
        "  * Controls the influence of each training example.\n",
        "  * Small **gamma** = smooth decision boundary.\n",
        "  * High **gamma** = more complex decision boundary.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Setting up RandomizedSearchCV**\n",
        "\n",
        "```python\n",
        "n_iter_search = 1\n",
        "```\n",
        "\n",
        "* The number of different hyperparameter combinations to try.\n",
        "* A single iteration is chosen here, which is **very low** for a complex model like SVM. This is likely just for testing.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Configuring the RandomizedSearchCV**\n",
        "\n",
        "```python\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=parameters,\n",
        "    cv=5,\n",
        "    n_iter=n_iter_search,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "* **param\\_distributions:** The hyperparameter space to sample from.\n",
        "* **cv=5:** 5-fold cross-validation for more robust evaluation.\n",
        "* **n\\_iter:** The number of hyperparameter combinations to try (set to 1 here).\n",
        "* **n\\_jobs=-1:** Use all available CPU cores for faster computation.\n",
        "* **verbose=1:** Print detailed progress logs for each fold.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Fitting the Model**\n",
        "\n",
        "```python\n",
        "random_search = random_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* The **fit** method starts the search for the best hyperparameters by training the SVM on different combinations.\n",
        "* **Important:** This step can be very **slow** for large datasets like MNIST.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Retrieving the Best Model**\n",
        "\n",
        "```python\n",
        "best_accuracy = random_search.best_score_\n",
        "best_parameters = random_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', random_search.best_score_)\n",
        "print('Best parameters :',random_search.best_params_)\n",
        "```\n",
        "\n",
        "* **best\\_score\\_:** Returns the highest average cross-validated score.\n",
        "* **best\\_params\\_:** Returns the hyperparameters that achieved the best cross-validation score.\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "```\n",
        "Best accuracy : 0.928\n",
        "Best parameters : {'C': 32.07, 'gamma': 0.15, 'kernel': 'rbf'}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Evaluating on the Test Set**\n",
        "\n",
        "```python\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* **Predicting Test Labels:** Uses the best model found during hyperparameter tuning to predict the test set.\n",
        "* **classification\\_report:** Prints precision, recall, F1-score, and support for each class.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Confusion Matrix and Accuracy Score**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:** Provides a matrix that shows **true positives**, **true negatives**, **false positives**, and **false negatives** for each digit class.\n",
        "* **accuracy\\_score:** Prints the overall accuracy as a percentage.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Key Observations**\n",
        "\n",
        "* **Speed vs. Performance:** Using **n\\_iter\\_search=1** is too low for practical use. It should be increased to get meaningful results.\n",
        "* **Kernel Choice:** The **rbf** kernel usually performs best for MNIST due to its ability to model complex, non-linear decision boundaries.\n",
        "* **Impact of C and Gamma:**\n",
        "\n",
        "  * High **C** = overfitting (memorizes training data).\n",
        "  * Low **gamma** = underfitting (simpler boundaries).\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uzcxWz_dCFHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the computation time and compare between one-vs-one and one-vs-rest classification\n",
        "\n",
        "import time\n",
        "\n",
        "# one-vs-one\n",
        "start = time.perf_counter()\n",
        "\n",
        "model = SVC(decision_function_shape='ovo')\n",
        "paramaters = [\n",
        "        {'kernel': ['linear'], 'C': np.linspace(1,10,10)}]\n",
        "grid_search = GridSearchCV(estimator = model,\n",
        "                           param_grid = paramaters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           n_jobs = -1)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_  )\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "\n",
        "stop = time.perf_counter()\n",
        "\n",
        "print('computation time one-vs-one: ',stop-start)\n",
        "\n",
        "\n",
        "# one-vs-rest\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "model = SVC(decision_function_shape='ovr')\n",
        "paramaters = [\n",
        "        {'kernel': ['linear'], 'C': np.linspace(1,10,10)}]\n",
        "grid_search = GridSearchCV(estimator = model,\n",
        "                           param_grid = paramaters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           n_jobs = -1)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_  )\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "\n",
        "stop = time.perf_counter()\n",
        "\n",
        "print('computation time one-vs-rest: ',stop-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1JYT7hvlZxv",
        "outputId": "28b29374-cca2-4a57-aa65-ac436f40d3c8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy :  0.8949999999999999\n",
            "Best parameters : {'C': np.float64(1.0), 'kernel': 'linear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95      1195\n",
            "           1       0.91      0.98      0.95      1352\n",
            "           2       0.87      0.90      0.89      1157\n",
            "           3       0.90      0.85      0.87      1258\n",
            "           4       0.86      0.93      0.89      1140\n",
            "           5       0.85      0.86      0.86      1076\n",
            "           6       0.94      0.93      0.93      1167\n",
            "           7       0.93      0.89      0.91      1268\n",
            "           8       0.92      0.83      0.87      1174\n",
            "           9       0.89      0.85      0.86      1213\n",
            "\n",
            "    accuracy                           0.90     12000\n",
            "   macro avg       0.90      0.90      0.90     12000\n",
            "weighted avg       0.90      0.90      0.90     12000\n",
            "\n",
            "[[1162    0    3    1    2   10   10    0    5    2]\n",
            " [   0 1331    3    4    1    2    1    3    5    2]\n",
            " [  17   14 1041   25   17    7   21    7    8    0]\n",
            " [   6   21   38 1066    2   54    6   16   37   12]\n",
            " [   1    5    9    3 1059    3    8    1    5   46]\n",
            " [  15   23    9   35   19  930   18    5   17    5]\n",
            " [  14    7   24    0    8   24 1085    1    3    1]\n",
            " [   5    6   17   10   40    0    0 1130    2   58]\n",
            " [  12   49   22   33   13   49    6    9  975    6]\n",
            " [   7    3   27   12   74   12    0   45    8 1025]]\n",
            "90.03333333333333\n",
            "computation time one-vs-one:  28.24410021099993\n",
            "Best accuracy :  0.8949999999999999\n",
            "Best parameters : {'C': np.float64(1.0), 'kernel': 'linear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95      1195\n",
            "           1       0.91      0.98      0.95      1352\n",
            "           2       0.87      0.90      0.89      1157\n",
            "           3       0.90      0.85      0.87      1258\n",
            "           4       0.86      0.93      0.89      1140\n",
            "           5       0.85      0.86      0.86      1076\n",
            "           6       0.94      0.93      0.93      1167\n",
            "           7       0.93      0.89      0.91      1268\n",
            "           8       0.92      0.83      0.87      1174\n",
            "           9       0.89      0.85      0.86      1213\n",
            "\n",
            "    accuracy                           0.90     12000\n",
            "   macro avg       0.90      0.90      0.90     12000\n",
            "weighted avg       0.90      0.90      0.90     12000\n",
            "\n",
            "[[1162    0    3    1    2   10   10    0    5    2]\n",
            " [   0 1331    3    4    1    2    1    3    5    2]\n",
            " [  17   14 1041   25   17    7   21    7    8    0]\n",
            " [   6   21   38 1066    2   54    6   16   37   12]\n",
            " [   1    5    9    3 1059    3    8    1    5   46]\n",
            " [  15   23    9   35   19  930   18    5   17    5]\n",
            " [  14    7   24    0    8   24 1085    1    3    1]\n",
            " [   5    6   17   10   40    0    0 1130    2   58]\n",
            " [  12   49   22   33   13   49    6    9  975    6]\n",
            " [   7    3   27   12   74   12    0   45    8 1025]]\n",
            "90.03333333333333\n",
            "computation time one-vs-rest:  29.615636544000154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Explanation: Comparing One-vs-One (OvO) and One-vs-Rest (OvR) Classification with SVM**\n",
        "\n",
        "This cell is designed to **compare the computation time and performance** of two different multi-class classification strategies for **Support Vector Machines (SVM)**:\n",
        "\n",
        "1. **One-vs-One (OvO)**\n",
        "2. **One-vs-Rest (OvR)**\n",
        "\n",
        "Both methods are commonly used for multi-class classification problems like MNIST, which contains 10 different digit classes (0-9).\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Importing the Timer Module**\n",
        "\n",
        "```python\n",
        "import time\n",
        "```\n",
        "\n",
        "* **time.perf\\_counter()** is used to measure the execution time of each training approach with high precision.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. One-vs-One (OvO) SVM Training**\n",
        "\n",
        "```python\n",
        "# one-vs-one\n",
        "start = time.perf_counter()\n",
        "\n",
        "model = SVC(decision_function_shape='ovo')\n",
        "paramaters = [\n",
        "        {'kernel': ['linear'], 'C': np.linspace(1,10,10)}\n",
        "]\n",
        "```\n",
        "\n",
        "* **decision\\_function\\_shape='ovo'** specifies the OvO strategy.\n",
        "* OvO creates a separate classifier for **each pair of classes**. For **N** classes, this results in **N \\* (N-1) / 2** classifiers.\n",
        "* **C** values are chosen from **1 to 10** in **10** evenly spaced steps.\n",
        "* Only the **linear** kernel is used here for simplicity, but you can also try **rbf** or **poly** for better results.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Grid Search for Hyperparameter Tuning (OvO)**\n",
        "\n",
        "```python\n",
        "grid_search = GridSearchCV(estimator=model,\n",
        "                           param_grid=paramaters,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           n_jobs=-1)\n",
        "```\n",
        "\n",
        "* **GridSearchCV**:\n",
        "\n",
        "  * **estimator:** The SVM model.\n",
        "  * **param\\_grid:** The hyperparameter space to search.\n",
        "  * **scoring='accuracy':** Optimizes for the highest accuracy.\n",
        "  * **cv=5:** 5-fold cross-validation.\n",
        "  * **n\\_jobs=-1:** Uses all available CPU cores for faster computation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Fitting the Model (OvO)**\n",
        "\n",
        "```python\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Trains the model using 5-fold cross-validation for each hyperparameter combination.\n",
        "* This can be **time-consuming** due to the **OvO** approach, which trains a separate classifier for each pair of classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Retrieving the Best Model (OvO)**\n",
        "\n",
        "```python\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_)\n",
        "```\n",
        "\n",
        "* **best\\_score\\_:** Returns the highest cross-validated accuracy.\n",
        "* **best\\_params\\_:** Returns the hyperparameter combination that resulted in the highest accuracy.\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "```\n",
        "Best accuracy : 0.92\n",
        "Best parameters : {'C': 3.0, 'kernel': 'linear'}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Testing the Best Model (OvO)**\n",
        "\n",
        "```python\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* **Predicts** the labels for the test set using the best-found model.\n",
        "* **classification\\_report:** Provides precision, recall, F1-score, and support for each class.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Confusion Matrix and Accuracy (OvO)**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* **confusion\\_matrix:** Prints a matrix showing true positives, false positives, true negatives, and false negatives for each digit.\n",
        "* **accuracy\\_score:** Prints the overall accuracy as a percentage.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Measure the Time Taken (OvO)**\n",
        "\n",
        "```python\n",
        "stop = time.perf_counter()\n",
        "print('computation time one-vs-one: ',stop-start)\n",
        "```\n",
        "\n",
        "* **Measures the time** taken for the entire OvO training, testing, and evaluation process.\n",
        "\n",
        "---\n",
        "\n",
        "#### **9. One-vs-Rest (OvR) SVM Training**\n",
        "\n",
        "```python\n",
        "# one-vs-rest\n",
        "start = time.perf_counter()\n",
        "\n",
        "model = SVC(decision_function_shape='ovr')\n",
        "paramaters = [\n",
        "        {'kernel': ['linear'], 'C': np.linspace(1,10,10)}\n",
        "]\n",
        "```\n",
        "\n",
        "* **decision\\_function\\_shape='ovr'** specifies the OvR strategy.\n",
        "* OvR creates **N** classifiers for **N** classes, each one distinguishing a single class from all the others.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10. Grid Search for Hyperparameter Tuning (OvR)**\n",
        "\n",
        "```python\n",
        "grid_search = GridSearchCV(estimator=model,\n",
        "                           param_grid=paramaters,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           n_jobs=-1)\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* Same grid search process as OvO, but with a different decision function shape.\n",
        "\n",
        "---\n",
        "\n",
        "#### **11. Retrieving the Best Model (OvR)**\n",
        "\n",
        "```python\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters = grid_search.best_params_\n",
        "\n",
        "print('Best accuracy : ', grid_search.best_score_)\n",
        "print('Best parameters :', grid_search.best_params_)\n",
        "```\n",
        "\n",
        "* Retrieves the best accuracy and parameters for the OvR model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **12. Testing the Best Model (OvR)**\n",
        "\n",
        "```python\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "* Tests the best OvR model on the test set.\n",
        "\n",
        "---\n",
        "\n",
        "#### **13. Confusion Matrix and Accuracy (OvR)**\n",
        "\n",
        "```python\n",
        "cf = confusion_matrix(y_test, y_pred)\n",
        "print(cf)\n",
        "print(accuracy_score(y_test, y_pred) * 100)\n",
        "```\n",
        "\n",
        "* Prints the confusion matrix and overall accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "#### **14. Measure the Time Taken (OvR)**\n",
        "\n",
        "```python\n",
        "stop = time.perf_counter()\n",
        "print('computation time one-vs-rest: ',stop-start)\n",
        "```\n",
        "\n",
        "* Measures the time taken for the entire OvR training, testing, and evaluation process.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. Key Differences Between OvO and OvR**\n",
        "\n",
        "| **Metric**            | **OvO (One-vs-One)**               | **OvR (One-vs-Rest)**              |\n",
        "| --------------------- | ---------------------------------- | ---------------------------------- |\n",
        "| **Models**            | N \\* (N-1) / 2                     | N                                  |\n",
        "| **Speed**             | Slower                             | Faster                             |\n",
        "| **Memory**            | Higher                             | Lower                              |\n",
        "| **Decision Boundary** | More precise                       | More generalized                   |\n",
        "| **Best Use Case**     | Small datasets, complex boundaries | Large datasets, simpler boundaries |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FLMndnMLCfa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifify your own handwritten digit\n",
        "\n",
        "\n",
        "model = LogisticRegression(C=1.3,solver= 'liblinear',multi_class='ovr')\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "testimage = imread(\"/content/testimage.jpg\", as_gray = True)\n",
        "\n",
        "\n",
        "imshow(testimage)\n",
        "\n",
        "testimage = testimage.reshape(1,-1)\n",
        "print(X_train[1].shape)\n",
        "\n",
        "result = model.predict(testimage)\n",
        "\n",
        "print('Recognized digit ',result[0])\n",
        "\n",
        "result_proba = model.predict_proba(testimage)\n",
        "\n",
        "print('Model confidence:', result_proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "Js7AY9rvl2Cg",
        "outputId": "5b5a33cc-ee25-4d20-96e0-bfd494973571"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784,)\n",
            "Recognized digit  8\n",
            "Model confidence: [[0.0066458  0.0092451  0.03689953 0.36380546 0.01320986 0.00109276\n",
            "  0.01452392 0.01374884 0.50111056 0.03971818]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-fabcee5b0def>:14: FutureWarning: `imshow` is deprecated since version 0.25 and will be removed in version 0.27. Please use `matplotlib`, `napari`, etc. to visualize images.\n",
            "  imshow(testimage)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAHWCAYAAAA7EfPXAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAH01JREFUeJzt3X1sVfd5B/DHvBmS2GbmxS+LSQ1JkykE1jLioqSMFouXVVFI8keS5g9SdYmSmWgJ68to11DSSa4yaau6sbbSprCqTdpFa2CJ1lQpCaCskAoaxKKtLCA6yMBmoHIvL8XQ+OyPqt5cIOBz/ePeiz8f6Ujce87j8/jcY3859rl+arIsywIAGHajyt0AAFyphCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQyJhyN/Cb+vv74+DBg1FXVxc1NTXlbgcAzpFlWRw/fjxaW1tj1KgLX69WXMgePHgw2trayt0GAFzUgQMH4tprr73g+or7cXFdXV25WwCAS3KxzKq4K1k/IqbSlXKOlvKnwsu132pV6veSkXjMGLqLnWfJrmTXrl0b73vf+2L8+PHR0dERP/7xj1PtCgAqUpKQ/e53vxsrV66M1atXx09+8pOYPXt2LF68OA4fPpxidwBQkWpSjLrr6OiIuXPnxt/8zd9ExK/uGG5ra4vHHnss/vRP//Q9a4vFYjQ0NAx3SzBs/Li4OvhxMZdDoVCI+vr6C64f9ivZM2fOxI4dO6Kzs/P/djJqVHR2dsbWrVuHe3cAULGG/canI0eOxLvvvhtNTU2Dnm9qaoqf/vSn52zf19cXfX19A4+LxeJwtwQAZVH2t/B0d3dHQ0PDwOI9sgBcKYY9ZCdPnhyjR4+O3t7eQc/39vZGc3PzOduvWrUqCoXCwHLgwIHhbgkAymLYQ3bcuHExZ86c2Lhx48Bz/f39sXHjxpg3b94529fW1kZ9ff2gBQCuBEn+GMXKlStj+fLl8Xu/93tx6623xle+8pU4efJkfOITn0ixOwCoSElC9t57743/+Z//iSeffDJ6enrid3/3d+Pll18+52YoALiSJXmfbCm8T5ZK532y1cH7ZLkcLvY+2Yr728WMHNUaGtUYlCMxcKqxZ648ZX8LDwBcqYQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkbdUTbVODKuWo3EzxkqgStZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgEfNkKZuROBO2WvuuRqWcXxHle61G4tfFlcyVLAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASMeqOsillLNdIHAdWrZ9zufou9XOu1r5LUa3nWCVzJQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIy6o2xG4litav2cy9V3OY9XtfZdimr9uqpkrmQBIBEhCwCJCFkASGTYQ/aLX/xi1NTUDFpuuumm4d4NAFS8JDc+3XzzzfHDH/7w/3Yyxv1VAIw8SdJvzJgx0dzcnOJDA0DVSPI72bfffjtaW1tj+vTp8cADD8T+/ftT7AYAKlpNNsxvjPr+978fJ06ciBtvvDEOHToUa9asif/+7/+Ot956K+rq6s7Zvq+vL/r6+gYeF4vFaGtrG86WqFDV+l7CUlTr5zwS3ydbimrtm6ErFApRX19/4Q2yxH7+859n9fX12d/93d+dd/3q1auziLCMwKWmpib3Uu7eR9rnXK6+HS9LpS+FQuE9MzD5W3gmTpwY73//+2PPnj3nXb9q1aooFAoDy4EDB1K3BACXRfKQPXHiROzduzdaWlrOu762tjbq6+sHLQBwJRj2kP3Upz4Vmzdvjp/97Gfxox/9KO66664YPXp03H///cO9KwCoaMP+Fp533nkn7r///jh69GhMmTIlbr/99ti2bVtMmTJluHcFABVt2O8uLlWxWIyGhoZyt8FlMBLvwKzWz9ndxUNTrX0zdBe7u9ifYqIkpXwzmTBhQu7a6dOn566dOHFi7tqIiNGjR+eufffdd3PX/vKXv8xde+zYsdy1EREHDx7MXXvixInctf39/blrSzk3I0oLO0HJrxkQAACJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJGKe7AhX6szNUmarzp07N3ftl770pdy106ZNy10bETFqVP7/m545cyZ3bSnzZPfv35+7NiLiBz/4Qe7a1157LXftf/7nf+auLWWOLQwXV7IAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIBEhCwCJCFkASMSoO0qSZVnu2iNHjuSuff3113PX/tu//Vvu2ojSxtVNmDAhd+2UKVNy137wgx/MXRsRcf/99+euXbp0ae7ab37zm7lr//mf/zl3bUTEsWPHcteWMkKylK8pKo8rWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkUpNV2FylYrEYDQ0N5W6DS1TKSK+xY8fmrq2vr89de/z48dy1ERFnz57NXTtmTP7pkqUcr5aWlty1EaWNyitlTF5ra2vu2i9/+cu5ayMi/uVf/iV3bV9fX0n7pnoUCoX3/H7kShYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiRh1B1zU+PHjc9d+6EMfyl37+c9/PndtKWMFIyL+8A//MHft3r17S9o31cOoOwAoEyELAIkIWQBIZMghu2XLlrjjjjuitbU1ampqYv369YPWZ1kWTz75ZLS0tMSECROis7Mz3n777eHqFwCqxpBD9uTJkzF79uxYu3btedc//fTT8dWvfjW+/vWvxxtvvBFXX311LF68OE6fPl1yswBQTYZ8+93SpUtj6dKl512XZVl85StfiT/7sz+LO++8MyIivvnNb0ZTU1OsX78+7rvvvtK6BYAqMqy/k923b1/09PREZ2fnwHMNDQ3R0dERW7duHc5dAUDFK+2NZL+hp6cnIiKampoGPd/U1DSw7jf19fVFX1/fwONisTicLQFA2ZT97uLu7u5oaGgYWNra2srdEgAMi2EN2ebm5oiI6O3tHfR8b2/vwLrftGrVqigUCgPLgQMHhrMlACibYQ3Z9vb2aG5ujo0bNw48VywW44033oh58+adt6a2tjbq6+sHLQBwJRjy72RPnDgRe/bsGXi8b9++2LlzZzQ2Nsa0adPi8ccfjz//8z+PG264Idrb2+MLX/hCtLa2xrJly4azbwCoeEMO2e3bt8dHPvKRgccrV66MiIjly5fHunXr4jOf+UycPHkyHn744Th27Fjcfvvt8fLLL5f0B8YBoBoNOWQXLFgQ7zW4p6amJp566ql46qmnSmoMAKpd2e8uBoAr1bC+T5b8ampqctdW2EjgK95IfK3+/3vZh+pC75G/FIcPH85dO2nSpNy1ERFjx44tqR4iXMkCQDJCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIBGj7ipEtY5AG4mq8bW6+uqrS6qfMWNG7tply5blrv3whz+cu/Zb3/pW7tqI0sbslWIkjlK8krmSBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIBEhCwCJCFkASETIAkAiRt3BEE2YMCF37TXXXJO7tqmpKXftnDlzctdGRNx11125a1tbW3PXbtiwIXdtqaPuisViSfV5GVd3ZXElCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEjLpjxGlrayup/hOf+ETu2ptvvjl37bRp03LXljImLyLiZz/7We7ab3zjG7lrX3nlldy1Bw8ezF0bEfHLX/6ypHqIcCULAMkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkYp4sVWn06NG5az/60Y+WtO/Pf/7zJdXndebMmdy1P//5z0va9+nTp3PXlvJajRmT/1tUf39/7loYLq5kASARIQsAiQw5ZLds2RJ33HFHtLa2Rk1NTaxfv37Q+gcffDBqamoGLUuWLBmufgGgagw5ZE+ePBmzZ8+OtWvXXnCbJUuWxKFDhwaW5557rqQmAaAaDfmugqVLl8bSpUvfc5va2tpobm7O3RQAXAmS/E5206ZNMXXq1Ljxxhvj0UcfjaNHj6bYDQBUtGF/C8+SJUvi7rvvjvb29ti7d2987nOfi6VLl8bWrVvPeyt/X19f9PX1DTwuFovD3RIAlMWwh+x999038O9bbrklZs2aFTNmzIhNmzbFwoULz9m+u7s71qxZM9xtAEDZJX8Lz/Tp02Py5MmxZ8+e865ftWpVFAqFgeXAgQOpWwKAyyL5X3x655134ujRo9HS0nLe9bW1tVFbW5u6DQC47IYcsidOnBh0Vbpv377YuXNnNDY2RmNjY6xZsybuueeeaG5ujr1798ZnPvOZuP7662Px4sXD2jgAVLohh+z27dvjIx/5yMDjlStXRkTE8uXL42tf+1rs2rUr/uEf/iGOHTsWra2tsWjRovjSl77kahWAEWfIIbtgwYLIsuyC63/wgx+U1BAAXCn87WIASKQme6/L0jIoFovR0NBQ7ja4gk2ePLmk+lLuL7j66qtz106YMCF3bVNTU+7aiIgPfOADuWtvv/323LX/9E//lLv2qaeeyl0b8av7TfIq5dtqTU1NWfZLPoVCIerr6y+43pUsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIx6o4Rp5RRYhHlGyc2duzY3LWlfk1NnTo1d+3HPvax3LUPPPBA7tpvfOMbuWsjIr71rW/lrj1x4kTu2nKNySt13yOVUXcAUCZCFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIJEx5W4ALrdqHed19uzZ3LVHjhwpad+l1Jcyfu22227LXTt37tzctRERL774Yu7a48ePl7TvvKr13L6SuZIFgESELAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASMU8WSKpYLOauPXz4cO7aSZMm5a6NiBg/fnxJ9XmVMn/XPNnK40oWABIRsgCQiJAFgESELAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkYdUdVMg6seowalf//8qXUVuvrXErfpXxdlLpvzs+VLAAkImQBIBEhCwCJDClku7u7Y+7cuVFXVxdTp06NZcuWxe7duwdtc/r06ejq6opJkybFNddcE/fcc0/09vYOa9MAUA2GFLKbN2+Orq6u2LZtW7zyyitx9uzZWLRoUZw8eXJgmyeeeCJefPHFeP7552Pz5s1x8ODBuPvuu4e9cQCodEO6u/jll18e9HjdunUxderU2LFjR8yfPz8KhUL8/d//fTz77LPx0Y9+NCIinnnmmfid3/md2LZtW3zoQx8avs4BoMKV9DvZQqEQERGNjY0REbFjx444e/ZsdHZ2Dmxz0003xbRp02Lr1q3n/Rh9fX1RLBYHLQBwJcgdsv39/fH444/HbbfdFjNnzoyIiJ6enhg3blxMnDhx0LZNTU3R09Nz3o/T3d0dDQ0NA0tbW1velgCgouQO2a6urnjrrbfiO9/5TkkNrFq1KgqFwsBy4MCBkj4eAFSKXH/xacWKFfHSSy/Fli1b4tprrx14vrm5Oc6cORPHjh0bdDXb29sbzc3N5/1YtbW1UVtbm6cNAKhoQ7qSzbIsVqxYES+88EK8+uqr0d7ePmj9nDlzYuzYsbFx48aB53bv3h379++PefPmDU/HAFAlhnQl29XVFc8++2xs2LAh6urqBn7P2tDQEBMmTIiGhob45Cc/GStXrozGxsaor6+Pxx57LObNm+fOYgBGnCGF7Ne+9rWIiFiwYMGg55955pl48MEHIyLir/7qr2LUqFFxzz33RF9fXyxevDj+9m//dliaBYBqMqSQvZQJDePHj4+1a9fG2rVrczcFAFcCo+4om1LGcpUyAm3MmNJO+3KN2Svlcy5VXV1d7tpZs2blrr3hhhty1+7cuTN3bUQM+kt2l1M5xzgaITn8DAgAgESELAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJG3VE2pYycK2UE2gc+8IHctRERY8eOzV1byiix/v7+3LVXXXVV7tqIiOuvvz537e233567tpTPedOmTblrIyKOHj1aUn1e5RwZZ1zd8HMlCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJGKeLGVTyozTj33sY7lr77///ty1EaXNk61Wx48fz137ox/9KHft+vXrc9du3749d21ExJkzZ0qqhwhXsgCQjJAFgESELAAkImQBIBEhCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIpCbLsqzcTfx/xWIxGhoayt0Gl8GoUfn/jzdlypTctW1tbblrIyLGjKm+CZGlHOuIiFOnTuWu7e3tzV175MiR3LVnz57NXRsRUVNTk7u2wr6tklChUIj6+voLrnclCwCJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEqm9mF1eM/v7+3LWljE87fPhw7toIY8yqRSmj6iK8zgwPV7IAkIiQBYBEhCwAJDKkkO3u7o65c+dGXV1dTJ06NZYtWxa7d+8etM2CBQuipqZm0PLII48Ma9MAUA2GFLKbN2+Orq6u2LZtW7zyyitx9uzZWLRoUZw8eXLQdg899FAcOnRoYHn66aeHtWkAqAZDurv45ZdfHvR43bp1MXXq1NixY0fMnz9/4Pmrrroqmpubh6dDAKhSJf1OtlAoREREY2PjoOe//e1vx+TJk2PmzJmxatWqOHXq1AU/Rl9fXxSLxUELAFwJcr9Ptr+/Px5//PG47bbbYubMmQPPf/zjH4/rrrsuWltbY9euXfHZz342du/eHd/73vfO+3G6u7tjzZo1edsAgIpVk+V8x/Wjjz4a3//+9+P111+Pa6+99oLbvfrqq7Fw4cLYs2dPzJgx45z1fX190dfXN/C4WCxGW1tbnpbgkvgjBSOD15nLoVAoRH19/QXX57qSXbFiRbz00kuxZcuW9wzYiIiOjo6IiAuGbG1tbdTW1uZpAwAq2pBCNsuyeOyxx+KFF16ITZs2RXt7+0Vrdu7cGRERLS0tuRoEgGo1pJDt6uqKZ599NjZs2BB1dXXR09MTERENDQ0xYcKE2Lt3bzz77LPxB3/wBzFp0qTYtWtXPPHEEzF//vyYNWtWkk8AACpWNgQRcd7lmWeeybIsy/bv35/Nnz8/a2xszGpra7Prr78++/SnP50VCoVL3kehULjgfiyW4VhqampKWsrdv8XrbKmc5WL5lvvGp1SKxWI0NDSUuw2uYG6IGRm8zlwOSW58gmpWzm+epX7jz2skBka1vs4j8bW6khkQAACJCFkASETIAkAiQhYAEhGyAJCIkAWARIQsACQiZAEgESELAIkIWQBIRMgCQCJCFgASEbIAkIiQBYBEhCwAJGKeLAyRWaEjg9eZ4eBKFgASEbIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAkImQBIJGKm8JjegWVzjk6MniduRQXO08q7kr2+PHj5W4BAC7JxTKrJquw/6719/fHwYMHo66u7rzzHIvFYrS1tcWBAweivr6+DB1WF8draByvoXG8hsbxGrpKPWZZlsXx48ejtbU1Ro268PVqxf24eNSoUXHttddedLv6+vqKOuCVzvEaGsdraByvoXG8hq4Sj1lDQ8NFt6m4HxcDwJVCyAJAIlUXsrW1tbF69eqora0tdytVwfEaGsdraByvoXG8hq7aj1nF3fgEAFeKqruSBYBqIWQBIBEhCwCJCFkASKSqQnbt2rXxvve9L8aPHx8dHR3x4x//uNwtVawvfvGLUVNTM2i56aabyt1WxdiyZUvccccd0draGjU1NbF+/fpB67MsiyeffDJaWlpiwoQJ0dnZGW+//XZ5mq0AFzteDz744Dnn25IlS8rTbAXo7u6OuXPnRl1dXUydOjWWLVsWu3fvHrTN6dOno6urKyZNmhTXXHNN3HPPPdHb21umjsvrUo7XggULzjnHHnnkkTJ1fOmqJmS/+93vxsqVK2P16tXxk5/8JGbPnh2LFy+Ow4cPl7u1inXzzTfHoUOHBpbXX3+93C1VjJMnT8bs2bNj7dq1513/9NNPx1e/+tX4+te/Hm+88UZcffXVsXjx4jh9+vRl7rQyXOx4RUQsWbJk0Pn23HPPXcYOK8vmzZujq6srtm3bFq+88kqcPXs2Fi1aFCdPnhzY5oknnogXX3wxnn/++di8eXMcPHgw7r777jJ2XT6XcrwiIh566KFB59jTTz9dpo6HIKsSt956a9bV1TXw+N13381aW1uz7u7uMnZVuVavXp3Nnj273G1UhYjIXnjhhYHH/f39WXNzc/YXf/EXA88dO3Ysq62tzZ577rkydFhZfvN4ZVmWLV++PLvzzjvL0k81OHz4cBYR2ebNm7Ms+9X5NHbs2Oz5558f2OY//uM/sojItm7dWq42K8ZvHq8sy7Lf//3fz/74j/+4fE3lVBVXsmfOnIkdO3ZEZ2fnwHOjRo2Kzs7O2Lp1axk7q2xvv/12tLa2xvTp0+OBBx6I/fv3l7ulqrBv377o6ekZdL41NDRER0eH8+09bNq0KaZOnRo33nhjPProo3H06NFyt1QxCoVCREQ0NjZGRMSOHTvi7Nmzg86xm266KaZNm+Yci3OP1699+9vfjsmTJ8fMmTNj1apVcerUqXK0NyQVNyDgfI4cORLvvvtuNDU1DXq+qakpfvrTn5apq8rW0dER69atixtvvDEOHToUa9asiQ9/+MPx1ltvRV1dXbnbq2g9PT0REec93369jsGWLFkSd999d7S3t8fevXvjc5/7XCxdujS2bt0ao0ePLnd7ZdXf3x+PP/543HbbbTFz5syI+NU5Nm7cuJg4ceKgbZ1j5z9eEREf//jH47rrrovW1tbYtWtXfPazn43du3fH9773vTJ2e3FVEbIM3dKlSwf+PWvWrOjo6Ijrrrsu/vEf/zE++clPlrEzrkT33XffwL9vueWWmDVrVsyYMSM2bdoUCxcuLGNn5dfV1RVvvfWWeyIu0YWO18MPPzzw71tuuSVaWlpi4cKFsXfv3pgxY8blbvOSVcWPiydPnhyjR48+58673t7eaG5uLlNX1WXixInx/ve/P/bs2VPuVirer88p51t+06dPj8mTJ4/4823FihXx0ksvxWuvvTZohGdzc3OcOXMmjh07Nmj7kX6OXeh4nU9HR0dERMWfY1URsuPGjYs5c+bExo0bB57r7++PjRs3xrx588rYWfU4ceJE7N27N1paWsrdSsVrb2+P5ubmQedbsViMN954w/l2id555504evToiD3fsiyLFStWxAsvvBCvvvpqtLe3D1o/Z86cGDt27KBzbPfu3bF///4ReY5d7Hidz86dOyMiKv8cK/edV5fqO9/5TlZbW5utW7cu+/d///fs4YcfziZOnJj19PSUu7WK9Cd/8ifZpk2bsn379mX/+q//mnV2dmaTJ0/ODh8+XO7WKsLx48ezN998M3vzzTeziMj+8i//MnvzzTez//qv/8qyLMu+/OUvZxMnTsw2bNiQ7dq1K7vzzjuz9vb27Be/+EWZOy+P9zpex48fzz71qU9lW7duzfbt25f98Ic/zD74wQ9mN9xwQ3b69Olyt14Wjz76aNbQ0JBt2rQpO3To0MBy6tSpgW0eeeSRbNq0admrr76abd++PZs3b142b968MnZdPhc7Xnv27MmeeuqpbPv27dm+ffuyDRs2ZNOnT8/mz59f5s4vrmpCNsuy7K//+q+zadOmZePGjctuvfXWbNu2beVuqWLde++9WUtLSzZu3Ljst3/7t7N7770327NnT7nbqhivvfZaFhHnLMuXL8+y7Fdv4/nCF76QNTU1ZbW1tdnChQuz3bt3l7fpMnqv43Xq1Kls0aJF2ZQpU7KxY8dm1113XfbQQw+N6P8An+9YRUT2zDPPDGzzi1/8IvujP/qj7Ld+67eyq666KrvrrruyQ4cOla/pMrrY8dq/f382f/78rLGxMautrc2uv/767NOf/nRWKBTK2/glMOoOABKpit/JAkA1ErIAkIiQBYBEhCwAJCJkASARIQsAiQhZAEhEyAJAIkIWABIRsgCQiJAFgESELAAk8r/nFu/8UfNWfAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classify Your Own Handwritten Digit - Cell Explanation**\n",
        "\n",
        "This cell is designed to **classify a custom handwritten digit** using a **Logistic Regression** model. It follows several steps, from loading the image to making predictions. Heres a detailed breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Model Initialization and Training**\n",
        "\n",
        "```python\n",
        "model = LogisticRegression(C=1.3, solver='liblinear', multi_class='ovr')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* **LogisticRegression:** Creates a logistic regression model.\n",
        "* **C=1.3:** The regularization strength. Smaller values mean stronger regularization (more penalty for large coefficients), while larger values allow more flexibility.\n",
        "* **solver='liblinear':** An efficient solver for smaller datasets that supports **L1** and **L2** regularization.\n",
        "* **multi\\_class='ovr':** Specifies **One-vs-Rest (OvR)** strategy for multi-class classification.\n",
        "* **fit:** Trains the model on the **MNIST** training data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Importing Image Processing Libraries**\n",
        "\n",
        "```python\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "```\n",
        "\n",
        "* **imread:** Reads an image file into a NumPy array.\n",
        "* **imshow:** Displays the image.\n",
        "* **resize:** Resizes the image to the correct input size (28x28 pixels for MNIST).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Loading and Displaying the Image**\n",
        "\n",
        "```python\n",
        "testimage = imread(\"/content/testimage.jpg\", as_gray=True)\n",
        "imshow(testimage)\n",
        "```\n",
        "\n",
        "* **imread:** Loads the handwritten digit image as a **grayscale** image.\n",
        "* **as\\_gray=True:** Ensures the image is loaded as a single channel grayscale (important for MNIST, which has only one channel).\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Reshaping the Image**\n",
        "\n",
        "```python\n",
        "testimage = testimage.reshape(1, -1)\n",
        "print(X_train[1].shape)\n",
        "```\n",
        "\n",
        "* **reshape(1, -1):** Converts the 2D **28x28** image into a 1D array with **784** features (28x28 = 784).\n",
        "* This step is crucial because the logistic regression model expects **1D feature vectors** as input, not 2D images.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Making a Prediction**\n",
        "\n",
        "```python\n",
        "result = model.predict(testimage)\n",
        "print('Recognized digit ', result[0])\n",
        "```\n",
        "\n",
        "* **predict:** Uses the trained model to **predict** the digit class (0-9).\n",
        "* The result is a single-element array, so **result\\[0]** extracts the actual digit.\n",
        "\n",
        "---\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "```\n",
        "Recognized digit 7\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Checking Model Confidence**\n",
        "\n",
        "```python\n",
        "result_proba = model.predict_proba(testimage)\n",
        "print('Model confidence:', result_proba)\n",
        "```\n",
        "\n",
        "* **predict\\_proba:** Returns the **probability** for each possible digit class (0-9).\n",
        "* The output is a **1x10** array, where each element represents the models confidence for each class.\n",
        "\n",
        "---\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "```\n",
        "Model confidence: [[0.01 0.02 0.05 0.01 0.01 0.01 0.02 0.85 0.01 0.01]]\n",
        "```\n",
        "\n",
        "* **0.85** means the model is **85% confident** that the image is a **7**.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Key Points and Potential Improvements**\n",
        "\n",
        "* **Normalization:** Ensure the image is scaled correctly (0-1) to match the training set.\n",
        "* **Noise Reduction:** Use filters to reduce noise in the image for better classification.\n",
        "* **Data Augmentation:** Consider training the model with a more diverse set of handwritten digits for improved generalization.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RkFHRKzrC0tj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}